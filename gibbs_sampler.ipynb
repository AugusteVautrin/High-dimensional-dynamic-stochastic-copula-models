{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ccccc9",
   "metadata": {},
   "source": [
    "# Implementation of the Gibbs sampler from the section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fca90b",
   "metadata": {},
   "source": [
    "On creer notre state space modele lorsque conditionnellement à z et zeta. Deplus ce state space modèle est défini unique pour un actif car conditionnelement à z et zeta nos actif sont independant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a676f8",
   "metadata": {},
   "source": [
    "On ne pourra pas directement utilisé le mcmc.ParticleGibbs de particules car l'un des principales aventage de l'augmented Gibbs est que conditionnelement au facteur on peut faire chaque actif est indépendant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e94b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats, linalg\n",
    "import particles\n",
    "from particles import state_space_models as ssm\n",
    "from particles import distributions as dists\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Pour la barre de progression (optionnel mais recommandé)\n",
    "from particles import smoothing\n",
    "from particles.mcmc import CSMC # Indispensable pour le Particle Gibbs\n",
    "from particles import state_space_models as ssm\n",
    "from particles import distributions as dists\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import particles\n",
    "from particles import distributions\n",
    "from scipy import stats\n",
    "from particles import mcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3596e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentifiedLoadingSSM(ssm.StateSpaceModel):\n",
    "    \"\"\"\n",
    "    Modèle espace d'état avec correction MvNormal pour éviter les erreurs de broadcasting.\n",
    "    \"\"\"\n",
    "    def __init__(self, row_idx, p_factors, mu, phi, sigma_eta, z, zeta, data_x):\n",
    "        self.i = row_idx\n",
    "        self.p = p_factors\n",
    "        # Dimension effective\n",
    "        self.dim_state = min(row_idx + 1, p_factors)\n",
    "        \n",
    "        # Paramètres\n",
    "        self.mu = mu[:self.dim_state]\n",
    "        self.phi = phi[:self.dim_state]\n",
    "        self.sigma_eta = sigma_eta[:self.dim_state]\n",
    "        \n",
    "        self.z = z       \n",
    "        self.zeta = zeta \n",
    "        self.data_x = data_x \n",
    "\n",
    "    def PX0(self):\n",
    "        \"\"\"Distribution initiale : MvNormal\"\"\"\n",
    "        # Variance stationnaire\n",
    "        var_0 = self.sigma_eta**2 / (1 - self.phi**2)\n",
    "        # On crée une matrice de covariance diagonale (dim_state, dim_state)\n",
    "        cov_0 = np.diag(var_0)\n",
    "        \n",
    "        # Utilisation explicite de MvNormal pour gérer les vecteurs correctement\n",
    "        return distributions.MvNormal(loc=self.mu, cov=cov_0)\n",
    "    \n",
    "    def PX0(self):\n",
    "        \"\"\"Distribution initiale avec variance gonflée (Diffuse Prior)\"\"\"\n",
    "        # Le texte dit : N(mu, Sigma * 100)\n",
    "        # Ici self.sigma_eta correspond au Sigma du texte (variance du choc)\n",
    "        \n",
    "        # Option A: Interprétation stricte (Sigma * 100)\n",
    "        var_0 = self.sigma_eta**2 * 100.0\n",
    "        \n",
    "        # Option B: Variance stationnaire * 100 (souvent utilisé aussi)\n",
    "        # var_0 = (self.sigma_eta**2 / (1 - self.phi**2)) * 100.0\n",
    "        \n",
    "        # L'article semble suggérer l'Option A ou simplement une grande variance.\n",
    "        # Utilisons l'Option A qui est plus simple et \"diffuse\".\n",
    "        \n",
    "        cov_0 = np.diag(var_0)\n",
    "        return distributions.MvNormal(loc=self.mu, cov=cov_0)\n",
    "\n",
    "    def PX(self, t, xp):\n",
    "        \"\"\"Transition : MvNormal\"\"\"\n",
    "        # xp est (N, dim_state). mean sera (N, dim_state)\n",
    "        mean = self.mu + self.phi * (xp - self.mu)\n",
    "        \n",
    "        # Covariance du bruit de transition\n",
    "        cov_eta = np.diag(self.sigma_eta**2)\n",
    "        \n",
    "        return distributions.MvNormal(loc=mean, cov=cov_eta)\n",
    "\n",
    "    def PY(self, t, xp, x):\n",
    "        \"\"\"Vraisemblance\"\"\"\n",
    "        # x est de forme (N, dim_state)\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # 1. Reconstitution\n",
    "        real_loadings = np.zeros((N, self.p)) \n",
    "        real_loadings[:, :self.dim_state] = x\n",
    "        \n",
    "        # 2. Transformation Log -> Exp\n",
    "        if self.i < self.p:\n",
    "            real_loadings[:, self.i] = np.exp(x[:, self.i])\n",
    "            \n",
    "        # 3. Paramètres\n",
    "        z_t = self.z[t]\n",
    "        factor_comp = np.dot(real_loadings, z_t)\n",
    "        \n",
    "        lam_sq = np.sum(real_loadings**2, axis=1)\n",
    "        scaling = np.sqrt(1.0 + lam_sq)\n",
    "        sigma_eps = 1.0 / scaling\n",
    "        \n",
    "        zeta_sqrt = np.sqrt(self.zeta[t])\n",
    "        \n",
    "        mean_obs = zeta_sqrt * (factor_comp / scaling)\n",
    "        scale_obs = zeta_sqrt * sigma_eps\n",
    "        \n",
    "        # Retourne l'objet Distribution (nécessaire pour particles)\n",
    "        return distributions.Normal(loc=mean_obs, scale=scale_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75413783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorCopulaSampler:\n",
    "    def __init__(self, data, n_factors):\n",
    "        self.data = data\n",
    "        self.T, self.n = data.shape\n",
    "        self.p = n_factors\n",
    "        \n",
    "        # --- 1. Initialisation des Paramètres ---\n",
    "        self.theta = {\n",
    "            'mu': np.random.normal(0.4, 0.5, size=(self.n, self.p)),\n",
    "            'phi': np.random.uniform(0.5, 0.9, size=(self.n, self.p)),\n",
    "            'sigma': np.ones((self.n, self.p)) * 0.1\n",
    "        }\n",
    "        \n",
    "        # --- 2. Variables Latentes ---\n",
    "        self.z = np.random.normal(size=(self.T, self.p))\n",
    "        self.zeta = np.ones((self.T, self.n))\n",
    "        \n",
    "        # Stockage des états: (T, n, p)\n",
    "        self.latent_states = np.zeros((self.T, self.n, self.p))\n",
    "        self.lambdas = np.zeros((self.T, self.n, self.p))\n",
    "        \n",
    "        # Initialisation (Diagonale non nulle pour éviter log(0))\n",
    "        for i in range(self.p):\n",
    "            self.lambdas[:, i, i] = 0.5\n",
    "            self.latent_states[:, i, i] = np.log(0.5)\n",
    "            \n",
    "        # Flag pour savoir si c'est la toute première itération (SMC vs CSMC)\n",
    "        self.first_iter = True\n",
    "\n",
    "    def update_z(self):\n",
    "        \"\"\"Step 4: Gibbs standard pour z_t (Inchangé)\"\"\"\n",
    "        T, n, p = self.T, self.n, self.p\n",
    "        new_z = np.zeros((T, p))\n",
    "        I_p = np.eye(p)\n",
    "        \n",
    "        for t in range(T):\n",
    "            x_t = self.data[t]\n",
    "            lambda_t = self.lambdas[t]\n",
    "            zeta_t = self.zeta[t]\n",
    "            \n",
    "            lam_sq = np.sum(lambda_t**2, axis=1)\n",
    "            sigma_sq = 1.0 / (1.0 + lam_sq)\n",
    "            sigma = np.sqrt(sigma_sq)\n",
    "            \n",
    "            lambda_tilde = lambda_t * sigma[:, np.newaxis]\n",
    "            x_dot = x_t / np.sqrt(zeta_t)\n",
    "            \n",
    "            D_inv_diag = 1.0 / sigma_sq\n",
    "            Ct_Dinv = lambda_tilde.T * D_inv_diag[np.newaxis, :]\n",
    "            \n",
    "            prec_post = I_p + Ct_Dinv @ lambda_tilde\n",
    "            linear_term = Ct_Dinv @ x_dot\n",
    "            \n",
    "            try:\n",
    "                L = linalg.cholesky(prec_post, lower=True)\n",
    "                mu_post = linalg.cho_solve((L, True), linear_term)\n",
    "            except linalg.LinAlgError:\n",
    "                L = linalg.cholesky(prec_post + 1e-6*np.eye(p), lower=True)\n",
    "                mu_post = linalg.cho_solve((L, True), linear_term)\n",
    "                \n",
    "            epsilon = np.random.normal(size=p)\n",
    "            z_noise = linalg.solve_triangular(L.T, epsilon, lower=False)\n",
    "            new_z[t] = mu_post + z_noise\n",
    "            \n",
    "        self.z = new_z\n",
    "\n",
    "    def update_lambda_pg(self):\n",
    "        \"\"\"\n",
    "        Step 5: Particle Gibbs pour Lambda.\n",
    "        Gère rigoureusement le format [array([val]), ...] de la librairie particles.\n",
    "        \"\"\"\n",
    "        T, n = self.data.shape\n",
    "        p = self.p\n",
    "        \n",
    "        new_lambdas = np.zeros((T, n, p))\n",
    "        new_states = np.zeros((T, n, p))\n",
    "        \n",
    "        mu_vec = self.theta['mu']\n",
    "        phi_vec = self.theta['phi']\n",
    "        sigma_vec = self.theta['sigma']\n",
    "        \n",
    "        for i in range(n):\n",
    "            data_i = self.data[:, i]\n",
    "            \n",
    "            ssm_i = IdentifiedLoadingSSM(\n",
    "                row_idx=i, p_factors=p,\n",
    "                mu=mu_vec[i], phi=phi_vec[i], sigma_eta=sigma_vec[i],\n",
    "                z=self.z, zeta=self.zeta[:, i], data_x=data_i\n",
    "            )\n",
    "            \n",
    "            if self.first_iter:\n",
    "                cpf = particles.SMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                                  N=100, store_history=True)\n",
    "            else:\n",
    "                dim = ssm_i.dim_state\n",
    "                x_prev_numpy = self.latent_states[:, i, :dim]\n",
    "\n",
    "                x_prev = [x_prev_numpy[t] for t in range(T)]\n",
    "                \n",
    "                cpf = mcmc.CSMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                                N=100, xstar=x_prev)\n",
    "            \n",
    "            cpf.run()\n",
    "            \n",
    "\n",
    "            raw_traj_list = cpf.hist.extract_one_trajectory()\n",
    "            \n",
    "            # Conversion en Numpy : (T, dim)\n",
    "            traj = np.array(raw_traj_list)\n",
    "            \n",
    "            # Sécurités de dimension\n",
    "            dim = ssm_i.dim_state\n",
    "            \n",
    "            # Si traj est (T, 1, 1) ou autre bizarrerie, on force (T, dim)\n",
    "            if traj.ndim > 2:\n",
    "                traj = np.squeeze(traj) # Enlève les dimensions 1 inutiles\n",
    "            if traj.ndim == 1:\n",
    "                traj = traj[:, np.newaxis] # Force (T, 1)\n",
    "                \n",
    "            # Gestion du cas T+1 (état initial parfois inclus par particles)\n",
    "            if traj.shape[0] == T + 1:\n",
    "                traj = traj[1:]\n",
    "            \n",
    "            # Vérification finale\n",
    "            if traj.shape != (T, dim):\n",
    "                # On redimensionne brutalement si le nombre d'éléments est bon\n",
    "                traj = traj.reshape(T, dim)\n",
    "\n",
    "            # --- 3. Stockage ---\n",
    "            new_states[:, i, :dim] = traj\n",
    "            new_lambdas[:, i, :dim] = traj\n",
    "            \n",
    "            if i < p:\n",
    "                # Pour la diagonale, on prend la colonne 'i' du vecteur d'état\n",
    "                # traj[:, i] renvoie un vecteur (T,) ce qui est parfait pour l'assignation\n",
    "                new_lambdas[:, i, i] = np.exp(traj[:, i])\n",
    "                new_lambdas[:, i, dim:] = 0.0\n",
    "                \n",
    "        self.lambdas = new_lambdas\n",
    "        self.latent_states = new_states\n",
    "        self.first_iter = False\n",
    "\n",
    "\n",
    "    def update_lambda_pg_old4(self):\n",
    "        \"\"\"\n",
    "        Step 5: Particle Gibbs pour Lambda (Version Debug & Robuste).\n",
    "        \"\"\"\n",
    "        # On récupère T directement des données\n",
    "        T, n = self.data.shape\n",
    "        p = self.p\n",
    "        \n",
    "        new_lambdas = np.zeros((T, n, p))\n",
    "        new_states = np.zeros((T, n, p))\n",
    "        \n",
    "        mu_vec = self.theta['mu']\n",
    "        phi_vec = self.theta['phi']\n",
    "        sigma_vec = self.theta['sigma']\n",
    "        \n",
    "        print(f\"--- DEBUG: Start update_lambda_pg (T={T}) ---\")\n",
    "        \n",
    "        for i in range(n):\n",
    "            data_i = self.data[:, i]\n",
    "            \n",
    "            ssm_i = IdentifiedLoadingSSM(\n",
    "                row_idx=i, p_factors=p,\n",
    "                mu=mu_vec[i], phi=phi_vec[i], sigma_eta=sigma_vec[i],\n",
    "                z=self.z, zeta=self.zeta[:, i], data_x=data_i\n",
    "            )\n",
    "            \n",
    "            # --- LOGIQUE PARTICLE GIBBS ---\n",
    "            if self.first_iter:\n",
    "                cpf = particles.SMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                                  N=100, store_history=True)\n",
    "            else:\n",
    "                dim = ssm_i.dim_state\n",
    "                x_prev = self.latent_states[:, i, :dim]\n",
    "                cpf = CSMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                           N=100, xstar=x_prev)\n",
    "            \n",
    "            cpf.run()\n",
    "            \n",
    "            # Récupération de la trajectoire\n",
    "            traj_list = cpf.hist.backward_sampling_ON2(M=1)\n",
    "            raw_traj = traj_list[0] # Liste de T éléments\n",
    "            \n",
    "            # Conversion en numpy\n",
    "            traj = np.array(raw_traj)\n",
    "            \n",
    "            # --- DIAGNOSTIC ET CORRECTION ---\n",
    "            dim = ssm_i.dim_state\n",
    "            \n",
    "            # 1. Gestion des vecteurs plats (T,) -> (T, 1)\n",
    "            if traj.ndim == 1:\n",
    "                traj = traj[:, np.newaxis]\n",
    "            \n",
    "            # 2. Vérification de la taille totale\n",
    "            expected_size = T * dim\n",
    "            if traj.size != expected_size:\n",
    "                print(f\"ERROR Asset {i}: Size mismatch.\")\n",
    "                print(f\"  -> Data T={T}, Dim={dim} => Expected size {expected_size}\")\n",
    "                print(f\"  -> Got traj shape {traj.shape} => Size {traj.size}\")\n",
    "                # On tente de voir si c'est un problème de T+1 (état initial inclus ?)\n",
    "                if traj.size == (T + 1) * dim:\n",
    "                    print(\"  -> Detected T+1 elements. Truncating extra state.\")\n",
    "                    traj = traj[1:] if dim == 1 else traj[1:, :]\n",
    "                else:\n",
    "                    raise ValueError(f\"FATAL: Impossible to reshape trajectory for Asset {i}\")\n",
    "\n",
    "            # 3. Force le reshape final (T, dim)\n",
    "            try:\n",
    "                traj = traj.reshape(T, dim)\n",
    "            except ValueError:\n",
    "                print(f\"ERROR Asset {i}: Reshape failed.\")\n",
    "                print(f\"  -> Traj shape: {traj.shape}, Target: ({T}, {dim})\")\n",
    "                raise\n",
    "\n",
    "            # Stockage\n",
    "            new_states[:, i, :dim] = traj\n",
    "            new_lambdas[:, i, :dim] = traj\n",
    "            \n",
    "            if i < p:\n",
    "                new_lambdas[:, i, i] = np.exp(traj[:, i])\n",
    "                new_lambdas[:, i, dim:] = 0.0\n",
    "                \n",
    "        self.lambdas = new_lambdas\n",
    "        self.latent_states = new_states\n",
    "        self.first_iter = False\n",
    "        print(\"--- DEBUG: End update_lambda_pg ---\")\n",
    "    \n",
    "    def update_lambda_pg_old3(self):\n",
    "        \"\"\"\n",
    "        Step 5: Particle Gibbs pour Lambda (Correction Robuste).\n",
    "        \"\"\"\n",
    "        # On lit T directement depuis les données pour éviter les incohérences\n",
    "        T, n = self.data.shape\n",
    "        p = self.p\n",
    "        \n",
    "        new_lambdas = np.zeros((T, n, p))\n",
    "        new_states = np.zeros((T, n, p))\n",
    "        \n",
    "        mu_vec = self.theta['mu']\n",
    "        phi_vec = self.theta['phi']\n",
    "        sigma_vec = self.theta['sigma']\n",
    "        \n",
    "        for i in range(n):\n",
    "            data_i = self.data[:, i] # Shape (T,)\n",
    "            \n",
    "            ssm_i = IdentifiedLoadingSSM(\n",
    "                row_idx=i, p_factors=p,\n",
    "                mu=mu_vec[i], phi=phi_vec[i], sigma_eta=sigma_vec[i],\n",
    "                z=self.z, zeta=self.zeta[:, i], data_x=data_i\n",
    "            )\n",
    "            \n",
    "            # --- LOGIQUE PARTICLE GIBBS ---\n",
    "            if self.first_iter:\n",
    "                cpf = particles.SMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                                  N=100, store_history=True)\n",
    "            else:\n",
    "                dim = ssm_i.dim_state\n",
    "                x_prev = self.latent_states[:, i, :dim]\n",
    "                cpf = CSMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                           N=100, xstar=x_prev)\n",
    "            \n",
    "            cpf.run()\n",
    "            \n",
    "            # Récupération de la trajectoire\n",
    "            # backward_sampling_ON2 retourne une LISTE de trajectoires\n",
    "            traj_list = cpf.hist.backward_sampling_ON2(M=1)\n",
    "            raw_traj = traj_list[0] \n",
    "            \n",
    "            # Conversion en numpy\n",
    "            traj = np.array(raw_traj) \n",
    "            \n",
    "            # --- BLOC DE CORRECTION DE FORME ---\n",
    "            # Problème : MvNormal(d=1) retourne souvent (T,) au lieu de (T, 1)\n",
    "            # Problème : MvNormal(d=2) retourne (T, 2)\n",
    "            \n",
    "            if traj.ndim == 1:\n",
    "                # Cas dim=1 : (T,) -> (T, 1)\n",
    "                traj = traj[:, np.newaxis]\n",
    "            \n",
    "            # Vérification de sécurité (Debug)\n",
    "            dim = ssm_i.dim_state\n",
    "            if traj.shape != (T, dim):\n",
    "                # Si cela plante ici, c'est que T ou dim ne sont pas ceux attendus\n",
    "                print(f\"DEBUG Error Asset {i}: Expected ({T}, {dim}), got {traj.shape}\")\n",
    "                # Tentative de reshape forcé si le nombre d'éléments est bon\n",
    "                if traj.size == T * dim:\n",
    "                    traj = traj.reshape(T, dim)\n",
    "                else:\n",
    "                    raise ValueError(f\"Shape mismatch impossible à corriger pour l'actif {i}\")\n",
    "\n",
    "            # Stockage\n",
    "            new_states[:, i, :dim] = traj\n",
    "            new_lambdas[:, i, :dim] = traj\n",
    "            \n",
    "            if i < p:\n",
    "                new_lambdas[:, i, i] = np.exp(traj[:, i])\n",
    "                new_lambdas[:, i, dim:] = 0.0\n",
    "                \n",
    "        self.lambdas = new_lambdas\n",
    "        self.latent_states = new_states\n",
    "        self.first_iter = False\n",
    "\n",
    "\n",
    "    def update_lambda_pg_old2(self):\n",
    "        \"\"\"\n",
    "        Step 5: Particle Gibbs pour Lambda (Robuste aux dimensions).\n",
    "        \"\"\"\n",
    "        T, n, p = self.T, self.n, self.p\n",
    "        new_lambdas = np.zeros((T, n, p))\n",
    "        new_states = np.zeros((T, n, p))\n",
    "        \n",
    "        mu_vec = self.theta['mu']\n",
    "        phi_vec = self.theta['phi']\n",
    "        sigma_vec = self.theta['sigma']\n",
    "        \n",
    "        for i in range(n):\n",
    "            data_i = self.data[:, i]\n",
    "            \n",
    "            ssm_i = IdentifiedLoadingSSM(\n",
    "                row_idx=i, p_factors=p,\n",
    "                mu=mu_vec[i], phi=phi_vec[i], sigma_eta=sigma_vec[i],\n",
    "                z=self.z, zeta=self.zeta[:, i], data_x=data_i\n",
    "            )\n",
    "            \n",
    "            # --- LOGIQUE PARTICLE GIBBS ---\n",
    "            if self.first_iter:\n",
    "                cpf = particles.SMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                                  N=100, store_history=True)\n",
    "            else:\n",
    "                dim = ssm_i.dim_state\n",
    "                x_prev = self.latent_states[:, i, :dim]\n",
    "                cpf = CSMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                           N=100, xstar=x_prev)\n",
    "            \n",
    "            cpf.run()\n",
    "            \n",
    "            # Récupération de la trajectoire\n",
    "            traj_list = cpf.hist.backward_sampling_ON2(M=1)\n",
    "            \n",
    "            # --- CORRECTION DE DIMENSION ---\n",
    "            # 1. On extrait l'unique trajectoire de la liste\n",
    "            raw_traj = traj_list[0] \n",
    "            \n",
    "            # 2. Conversion en numpy\n",
    "            traj = np.array(raw_traj) # Peut être (T,) ou (T, dim)\n",
    "            \n",
    "            # 3. Force la dimension (T, dim) si elle est aplatie\n",
    "            if traj.ndim == 1:\n",
    "                traj = traj[:, np.newaxis]\n",
    "            \n",
    "            # Stockage\n",
    "            dim = ssm_i.dim_state\n",
    "            new_states[:, i, :dim] = traj\n",
    "            new_lambdas[:, i, :dim] = traj\n",
    "            \n",
    "            if i < p:\n",
    "                new_lambdas[:, i, i] = np.exp(traj[:, i])\n",
    "                new_lambdas[:, i, dim:] = 0.0\n",
    "                \n",
    "        self.lambdas = new_lambdas\n",
    "        self.latent_states = new_states\n",
    "        self.first_iter = False\n",
    "    def update_lambda_pg_old(self):\n",
    "        \"\"\"\n",
    "        Step 5: Particle Gibbs pour Lambda (Inspiré du code de N. Chopin).\n",
    "        Utilise CSMC (Conditional SMC) si une trajectoire précédente existe.\n",
    "        \"\"\"\n",
    "        T, n, p = self.T, self.n, self.p\n",
    "        new_lambdas = np.zeros((T, n, p))\n",
    "        new_states = np.zeros((T, n, p))\n",
    "        \n",
    "        mu_vec = self.theta['mu']\n",
    "        phi_vec = self.theta['phi']\n",
    "        sigma_vec = self.theta['sigma']\n",
    "        \n",
    "        for i in range(n):\n",
    "            # Données de l'actif i\n",
    "            data_i = self.data[:, i]\n",
    "            \n",
    "            # Modèle SSM pour l'actif i\n",
    "            ssm_i = IdentifiedLoadingSSM(\n",
    "                row_idx=i, p_factors=p,\n",
    "                mu=mu_vec[i], phi=phi_vec[i], sigma_eta=sigma_vec[i],\n",
    "                z=self.z, zeta=self.zeta[:, i], data_x=data_i\n",
    "            )\n",
    "            \n",
    "            # --- LOGIQUE PARTICLE GIBBS ---\n",
    "            # Si c'est la première itération, on n'a pas de trajectoire de référence -> SMC standard\n",
    "            # Sinon, on conditionne sur la trajectoire précédente (xstar) -> CSMC\n",
    "            \n",
    "            if self.first_iter:\n",
    "                # Premier passage : SMC Standard\n",
    "                cpf = particles.SMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                                  N=100, store_history=True)\n",
    "            else:\n",
    "                # Passages suivants : Conditional SMC\n",
    "                # On récupère la trajectoire précédente (xstar) pour cet actif\n",
    "                # Attention au format : CSMC attend une liste ou un array (T, dim)\n",
    "                dim = ssm_i.dim_state\n",
    "                x_prev = self.latent_states[:, i, :dim]\n",
    "                \n",
    "                # Instanciation du CSMC\n",
    "                cpf = CSMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                           N=100, xstar=x_prev) # xstar fixe la trajectoire de référence\n",
    "            \n",
    "            # Exécution\n",
    "            cpf.run()\n",
    "            \n",
    "            # Backward Sampling (O(N) ou O(N^2) selon dispo, ici via smoothing)\n",
    "            # Cela correspond au `cpf.hist.backward_sampling_ON2(1)` du code source\n",
    "            traj_list = cpf.hist.backward_sampling_ON2(1)\n",
    "            traj = np.array(traj_list)[:, :, 0] \n",
    "            \n",
    "            dim = ssm_i.dim_state\n",
    "            new_states[:, i, :dim] = traj\n",
    "            new_lambdas[:, i, :dim] = traj\n",
    "            \n",
    "            if i < p:\n",
    "                new_lambdas[:, i, i] = np.exp(traj[:, i])\n",
    "                new_lambdas[:, i, dim:] = 0.0\n",
    "                \n",
    "        self.lambdas = new_lambdas\n",
    "        self.latent_states = new_states\n",
    "        \n",
    "        self.first_iter = False\n",
    "\n",
    "    def update_sigma(self):\n",
    "        \"\"\"Step 6: Inverse Gamma\"\"\"\n",
    "        h_curr = self.latent_states[:-1]\n",
    "        h_next = self.latent_states[1:]\n",
    "        \n",
    "        mu = self.theta['mu'][np.newaxis, :, :]\n",
    "        phi = self.theta['phi'][np.newaxis, :, :]\n",
    "        \n",
    "        pred = mu + phi * (h_curr - mu)\n",
    "        resid = h_next - pred\n",
    "        sse = np.sum(resid**2, axis=0) \n",
    "        \n",
    "        alpha_post = 20.0 + (h_curr.shape[0] / 2.0)\n",
    "        beta_post = 0.25 + (sse / 2.0)\n",
    "        \n",
    "        self.theta['sigma'] = 1.0 / np.random.gamma(alpha_post, 1.0/beta_post)\n",
    "\n",
    "    def update_mu_phi(self):\n",
    "        \"\"\"Step 7: Mu et Phi\"\"\"\n",
    "        h_curr = self.latent_states[:-1]\n",
    "        h_next = self.latent_states[1:]\n",
    "        T_eff = h_curr.shape[0]\n",
    "        sigma_sq = self.theta['sigma']\n",
    "        \n",
    "        # --- Update MU ---\n",
    "        phi = self.theta['phi']\n",
    "        Y = h_next - phi[np.newaxis, :, :] * h_curr\n",
    "        X = 1.0 - phi\n",
    "        \n",
    "        prec_prior = 1.0 / 2.0 \n",
    "        prec_lik = T_eff * (X**2) / sigma_sq\n",
    "        prec_post = prec_prior + prec_lik\n",
    "        \n",
    "        mean_lik = (X / sigma_sq) * np.sum(Y, axis=0)\n",
    "        mean_post = (1.0/prec_post) * (prec_prior*0.4 + mean_lik)\n",
    "        \n",
    "        self.theta['mu'] = np.random.normal(mean_post, np.sqrt(1.0/prec_post))\n",
    "        \n",
    "        # --- Update PHI ---\n",
    "        mu = self.theta['mu']\n",
    "        X_c = h_curr - mu[np.newaxis, :, :]\n",
    "        Y_c = h_next - mu[np.newaxis, :, :]\n",
    "        \n",
    "        num = np.sum(X_c * Y_c, axis=0)\n",
    "        den = np.sum(X_c**2, axis=0)\n",
    "        \n",
    "        prec_prior_phi = 1.0 / 0.001\n",
    "        prec_lik_phi = den / sigma_sq\n",
    "        prec_post_phi = prec_prior_phi + prec_lik_phi\n",
    "        \n",
    "        mean_post_phi = (1.0/prec_post_phi) * (prec_prior_phi*0.985 + (num/sigma_sq))\n",
    "        std_post_phi = np.sqrt(1.0/prec_post_phi)\n",
    "        \n",
    "        a = (-1.0 - mean_post_phi) / std_post_phi\n",
    "        b = (1.0 - mean_post_phi) / std_post_phi\n",
    "        self.theta['phi'] = stats.truncnorm.rvs(a, b, loc=mean_post_phi, scale=std_post_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b33fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(T, n, p):\n",
    "    print(f\"Simulation: T={T}, n={n}, p={p}\")\n",
    "    true_mu = np.full((n, p), 0.5)\n",
    "    true_phi = np.full((n, p), 0.9)\n",
    "    true_sigma = np.full((n, p), 0.1)\n",
    "    \n",
    "    z = np.random.normal(size=(T, p))\n",
    "    h = np.zeros((T, n, p))\n",
    "    h[0] = true_mu\n",
    "    \n",
    "    # Simule AR(1) latent\n",
    "    for t in range(1, T):\n",
    "        h[t] = true_mu + true_phi*(h[t-1]-true_mu) + np.random.normal(0, np.sqrt(true_sigma))\n",
    "        \n",
    "    # Transforme en lambdas (Identification)\n",
    "    lambdas = h.copy()\n",
    "    for i in range(p):\n",
    "        lambdas[:, i, i] = np.exp(h[:, i, i])\n",
    "        lambdas[:, i, i+1:] = 0.0\n",
    "        \n",
    "    x = np.zeros((T, n))\n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            lam = lambdas[t, i]\n",
    "            scaling = np.sqrt(1 + np.sum(lam**2))\n",
    "            \n",
    "            # Modèle Factoriel avec scaling\n",
    "            # x = (lambda/scale)*z + (1/scale)*eps\n",
    "            mean = np.dot(lam/scaling, z[t])\n",
    "            std = 1.0 / scaling\n",
    "            x[t, i] = np.random.normal(mean, std)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7500e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# 1. Fonction helper définie EN DEHORS de la classe\n",
    "def process_single_asset(i, p, mu_i, phi_i, sigma_i, z, zeta_i, data_i, first_iter, x_prev_i=None):\n",
    "    \"\"\"Traite un seul actif : Crée le SSM, lance le SMC/CSMC et retourne la trajectoire.\"\"\"\n",
    "    \n",
    "    # Recréation du modèle local pour l'actif i\n",
    "    ssm_i = IdentifiedLoadingSSM(\n",
    "        row_idx=i, p_factors=p,\n",
    "        mu=mu_i, phi=phi_i, sigma_eta=sigma_i,\n",
    "        z=z, zeta=zeta_i, data_x=data_i\n",
    "    )\n",
    "    \n",
    "    # SMC ou CSMC\n",
    "    if first_iter:\n",
    "        cpf = particles.SMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                          N=100, store_history=True)\n",
    "    else:\n",
    "        # x_prev_i doit être déjà formaté en liste de arrays si nécessaire\n",
    "        cpf = mcmc.CSMC(fk=ssm.Bootstrap(ssm=ssm_i, data=data_i), \n",
    "                        N=100, xstar=x_prev_i)\n",
    "    \n",
    "    cpf.run()\n",
    "    \n",
    "    # Extraction (identique à votre code)\n",
    "    raw_traj_list = cpf.hist.extract_one_trajectory()\n",
    "    traj = np.array(raw_traj_list)\n",
    "    \n",
    "    # Nettoyage des dimensions\n",
    "    T = data_i.shape[0]\n",
    "    dim = ssm_i.dim_state\n",
    "    \n",
    "    if traj.ndim > 2: traj = np.squeeze(traj)\n",
    "    if traj.ndim == 1: traj = traj[:, np.newaxis]\n",
    "    if traj.shape[0] == T + 1: traj = traj[1:]\n",
    "    if traj.shape != (T, dim): traj = traj.reshape(T, dim)\n",
    "        \n",
    "    # Calcul des lambdas (exponentielle pour la diagonale)\n",
    "    lambdas_vals = np.zeros((T, p))\n",
    "    lambdas_vals[:, :dim] = traj\n",
    "    if i < p:\n",
    "        lambdas_vals[:, i] = np.exp(traj[:, i])\n",
    "        \n",
    "    return i, traj, lambdas_vals\n",
    "\n",
    "# 2. Modification de votre classe FactorCopulaSampler\n",
    "class FactorCopulaSampler(FactorCopulaSampler): # On étend ou on remplace la méthode\n",
    "    def update_lambda_pg_parallel(self, n_jobs=-1):\n",
    "        \"\"\"Version parallélisée de update_lambda_pg\"\"\"\n",
    "        T, n = self.data.shape\n",
    "        \n",
    "        # Préparation des arguments pour chaque actif\n",
    "        tasks = []\n",
    "        for i in range(n):\n",
    "            # Récupération de x_prev si ce n'est pas la première itération\n",
    "            x_prev = None\n",
    "            if not self.first_iter:\n",
    "                dim = min(i + 1, self.p)\n",
    "                x_prev_numpy = self.latent_states[:, i, :dim]\n",
    "                x_prev = [x_prev_numpy[t] for t in range(T)]\n",
    "            \n",
    "            tasks.append((\n",
    "                i, self.p, \n",
    "                self.theta['mu'][i], self.theta['phi'][i], self.theta['sigma'][i],\n",
    "                self.z, self.zeta[:, i], self.data[:, i], \n",
    "                self.first_iter, x_prev\n",
    "            ))\n",
    "            \n",
    "        # Exécution parallèle (n_jobs=-1 utilise tous les cœurs)\n",
    "        results = Parallel(n_jobs=n_jobs)(delayed(process_single_asset)(*t) for t in tasks)\n",
    "        \n",
    "        # Réassemblage des résultats\n",
    "        for i, traj, lam_vals in results:\n",
    "            dim = min(i + 1, self.p)\n",
    "            self.latent_states[:, i, :dim] = traj\n",
    "            self.lambdas[:, i, :] = lam_vals\n",
    "            \n",
    "        self.first_iter = False\n",
    "\n",
    "# Remplacez ensuite l'appel dans votre boucle principale :\n",
    "# sampler.update_lambda_pg_parallel(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: T=1000, n=100, p=1\n",
      "Démarrage du Gibbs Sampler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 100/500 [13:57<57:07,  8.57s/it] "
     ]
    }
   ],
   "source": [
    "# 1. Paramètres\n",
    "T_sim = 1000\n",
    "n_sim = 100  # 10 actifs\n",
    "p_sim = 1  # 2 facteurs\n",
    "N_ITER = 500 # Nombre d'itérations MCMC\n",
    "\n",
    "# 2. Données\n",
    "data = simulate_data(T_sim, n_sim, p_sim)\n",
    "\n",
    "# 3. Instanciation du Sampler\n",
    "sampler = FactorCopulaSampler(data, p_sim)\n",
    "\n",
    "# 4. Préparation du stockage (Trace)\n",
    "# On stocke l'historique de mu pour visualiser la convergence par exemple\n",
    "# Vous pouvez faire de même pour phi et sigma\n",
    "trace_mu = np.zeros((N_ITER, n_sim, p_sim))\n",
    "\n",
    "# 5. Boucle Principale\n",
    "print(\"Démarrage du Gibbs Sampler...\")\n",
    "for it in tqdm(range(N_ITER)):\n",
    "    \n",
    "    # A. Mises à jour séquentielles\n",
    "    # On appelle explicitement chaque étape dans l'ordre\n",
    "    sampler.update_z()          # Step 4\n",
    "    sampler.update_lambda_pg_parallel()  # Step 5\n",
    "    sampler.update_sigma()      # Step 6\n",
    "    sampler.update_mu_phi()     # Step 7\n",
    "    \n",
    "    # B. Stockage\n",
    "    # On sauvegarde une copie profonde des paramètres à chaque itération\n",
    "    trace_mu[it] = sampler.theta['mu'].copy()\n",
    "\n",
    "print(\"Terminé !\")\n",
    "\n",
    "# 6. Visualisation rapide\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(trace_mu[:, 0, 0], label=\"Mu (Actif 0, Facteur 0)\")\n",
    "plt.axhline(0.5, color='r', linestyle='--', label=\"Vraie valeur (approx)\")\n",
    "plt.title(\"Trace de convergence pour Mu\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9653910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.32725877,  1.58494868],\n",
       "       [ 0.66501141,  0.35413243],\n",
       "       [-0.0690377 , -1.62751025],\n",
       "       [-0.37322413,  0.66639706],\n",
       "       [ 0.42371289,  0.48340346],\n",
       "       [ 0.55279378,  1.03464789],\n",
       "       [ 0.21367942, -0.14229332],\n",
       "       [ 0.25405686,  0.07702556],\n",
       "       [ 0.16251059, -0.12677716],\n",
       "       [ 0.3647809 ,  0.31435562]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_mu[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0614e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qrt_foot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
