{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ccccc9",
   "metadata": {},
   "source": [
    "# Implementation of the Gibbs sampler from the section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74c247f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from particles import state_space_models as ssm\n",
    "from particles import distributions as dists\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import particles\n",
    "from particles import distributions\n",
    "from scipy import stats\n",
    "from particles import mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fca90b",
   "metadata": {},
   "source": [
    "On creer notre state space modele lorsque conditionnellement à z et zeta. Deplus ce state space modèle est défini unique pour un actif car conditionnelement à z et zeta nos actif sont independant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8842fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentifiedLoadingSSM(ssm.StateSpaceModel):\n",
    "    \"\"\"\n",
    "    Modèle espace d'état pour les chargements d'un actif 'i' avec contraintes d'identification.\n",
    "    \n",
    "    Règles d'identification :\n",
    "    - Pour les p premières lignes (i < p):\n",
    "        * La matrice est triangulaire inférieure (lambda_{ik} = 0 pour k > i).\n",
    "        * Les éléments diagonaux (lambda_{ii}) sont stricts positifs -> modélisés en log.\n",
    "    - Pour les lignes suivantes (i >= p):\n",
    "        * Pas de contrainte de structure ou de signe.\n",
    "        \n",
    "    Attributs:\n",
    "        row_idx (int): Indice de l'actif courant (i).\n",
    "        p_factors (int): Nombre total de facteurs.\n",
    "        dim_state (int): Dimension effective du vecteur d'état pour cet actif.\n",
    "    \"\"\"\n",
    "    def __init__(self, row_idx, p_factors, mu, phi, sigma_eta, z, zeta, data_x):\n",
    "        self.i = row_idx\n",
    "        self.p = p_factors\n",
    "        \n",
    "        # Dimension effective: Pour i < p, on ne simule que les i+1 premiers éléments.\n",
    "        self.dim_state = min(row_idx + 1, p_factors)\n",
    "        \n",
    "        # On ne conserve que les paramètres pertinents pour la dimension active\n",
    "        self.mu = mu[:self.dim_state]         # (dim_state,)\n",
    "        self.phi = phi[:self.dim_state]       # (dim_state,)\n",
    "        self.sigma_eta = sigma_eta[:self.dim_state] # (dim_state,)\n",
    "        \n",
    "        self.z = z       # Facteurs communs (T, p)\n",
    "        self.zeta = zeta # Variable de mélange (T,)\n",
    "        self.data_x = data_x # Données observées (T,)\n",
    "\n",
    "    def PX0(self):\n",
    "        \"\"\"Distribution initiale des états h_0 \"\"\"\n",
    "        # Variance stationnaire: sigma^2 / (1 - phi^2)\n",
    "        var_0 = self.sigma_eta**2 / (1 - self.phi**2)\n",
    "        \n",
    "        # Distribution indépendante sur chaque composante du vecteur d'état\n",
    "        return distributions.IndepProd(\n",
    "            distributions.Normal(loc=self.mu, scale=np.sqrt(var_0))\n",
    "        )\n",
    "\n",
    "    def PX(self, t, xp):\n",
    "        \"\"\"Transition des états h_t | h_{t-1} \"\"\"\n",
    "        # Dynamique AR(1) Gaussienne sur h_t\n",
    "        # Note: Si k=i (diagonale), h_t correspond au LOG-loading.\n",
    "        mean = self.mu + self.phi * (xp - self.mu)\n",
    "        return distributions.IndepProd(\n",
    "            distributions.Normal(loc=mean, scale=self.sigma_eta)\n",
    "        )\n",
    "\n",
    "    def PY(self, t, xp, x):\n",
    "        \"\"\"\n",
    "        Vraisemblance p(x_it | h_it, z_t, zeta_t) \n",
    "        Transforme l'état latent h_t en chargement lambda_t puis calcule la densité.\n",
    "        \"\"\"\n",
    "        # x est de forme (N_particules, dim_state)\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # 1. Reconstitution du vecteur de chargements complet (taille p)\n",
    "        real_loadings = np.zeros((N, self.p)) \n",
    "        \n",
    "        # Copie des états latents\n",
    "        real_loadings[:, :self.dim_state] = x\n",
    "        \n",
    "        # 2. Application de la contrainte de positivité (Log -> Exp)\n",
    "        # Uniquement pour les éléments diagonaux des p premiers actifs\n",
    "        if self.i < self.p:\n",
    "            # L'élément diagonal est le dernier de l'état actif (indice self.i)\n",
    "            # h_{ii} = log(lambda_{ii})  =>  lambda_{ii} = exp(h_{ii})\n",
    "            real_loadings[:, self.i] = np.exp(x[:, self.i])\n",
    "            \n",
    "            # Les éléments k > i restent à 0.0 (Triangulaire inf)\n",
    "            \n",
    "        # 3. Calcul de la composante factorielle et du scaling\n",
    "        z_t = self.z[t] # (p,)\n",
    "        \n",
    "        # Produit scalaire lambda' * z\n",
    "        factor_comp = np.dot(real_loadings, z_t) # (N,)\n",
    "        \n",
    "        # Norme au carré des lambda pour le scaling sigma_{it} \n",
    "        lam_sq = np.sum(real_loadings**2, axis=1) # (N,)\n",
    "        \n",
    "        # Scaling factor: sqrt(1 + lambda'lambda)\n",
    "        scaling = np.sqrt(1.0 + lam_sq)\n",
    "        \n",
    "        # Écart-type résiduel: sigma = 1 / scaling\n",
    "        sigma_eps = 1.0 / scaling\n",
    "        \n",
    "        # 4. Paramètres de la loi Normale de l'observation x_it\n",
    "        # x_it = sqrt(zeta) * ( (lambda/scaling)'z + (1/scaling)*eps )\n",
    "        zeta_sqrt = np.sqrt(self.zeta[t])\n",
    "        \n",
    "        mean_obs = zeta_sqrt * (factor_comp / scaling)\n",
    "        scale_obs = zeta_sqrt * sigma_eps\n",
    "        \n",
    "        return distributions.Normal(loc=mean_obs, scale=scale_obs).logpdf(self.data_x[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a676f8",
   "metadata": {},
   "source": [
    "On ne pourra pas directement utilisé le mcmc.ParticleGibbs de particules car l'un des principales aventage de l'augmented Gibbs est que conditionnelement au facteur on peut faire chaque actif est indépendant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "227df371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorCopulaPrior:\n",
    "    \"\"\"\n",
    "    Classe de Prior spécifique pour le modèle Factor Copula de Creal & Tsay.\n",
    "    Gère l'initialisation des paramètres matriciels (n x p).\n",
    "    \n",
    "    Priors de l'article [Section 4.2]:\n",
    "    - mu ~ N(0.4, 2) [cite: 362]\n",
    "    - phi ~ N(0.985, 0.001) tronqué sur (-1, 1) [cite: 363]\n",
    "    - sigma ~ InvGamma(20, 0.25) [cite: 363]\n",
    "    \"\"\"\n",
    "    def __init__(self, n, p):\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "        self.dtype = [\n",
    "            ('mu', 'float64', (n, p)),\n",
    "            ('phi', 'float64', (n, p)),\n",
    "            ('sigma', 'float64', (n, p))\n",
    "        ]\n",
    "\n",
    "    def sample_initial(self):\n",
    "        \"\"\"\n",
    "        Génère un dictionnaire theta avec les dimensions correctes (n, p).\n",
    "        \"\"\"\n",
    "        # 1. Mu ~ Normal(0.4, 2)\n",
    "        # Note: loc=0.4, scale=sqrt(2) approx 1.414\n",
    "        mu = np.random.normal(loc=0.4, scale=np.sqrt(2.0), size=(self.n, self.p))\n",
    "        \n",
    "        # 2. Phi ~ Normal(0.985, 0.001) Tronqué sur (-1, 1)\n",
    "        # scale = sqrt(0.001) approx 0.0316\n",
    "        loc_phi = 0.985\n",
    "        scale_phi = np.sqrt(0.001)\n",
    "        \n",
    "        # Calcul des bornes pour la normale tronquée standard\n",
    "        a_clip = (-1.0 - loc_phi) / scale_phi\n",
    "        b_clip = (1.0 - loc_phi) / scale_phi\n",
    "        \n",
    "        phi = stats.truncnorm.rvs(\n",
    "            a_clip, b_clip, \n",
    "            loc=loc_phi, scale=scale_phi, \n",
    "            size=(self.n, self.p)\n",
    "        )\n",
    "        \n",
    "        # 3. Sigma ~ Inverse Gamma(20, 0.25)\n",
    "        # En numpy/scipy: InvGamma(a, scale=b) équivaut à 1 / Gamma(a, scale=1/b)\n",
    "        alpha = 20.0\n",
    "        beta = 0.25\n",
    "        \n",
    "        # On tire d'abord dans une Gamma(alpha, scale=1/beta)\n",
    "        gamma_draws = np.random.gamma(shape=alpha, scale=1.0/beta, size=(self.n, self.p))\n",
    "        sigma = 1.0 / gamma_draws\n",
    "        \n",
    "        return {\n",
    "            'mu': mu,\n",
    "            'phi': phi,\n",
    "            'sigma': sigma\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa2b12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorCopulaGibbs(mcmc.GenericGibbs):\n",
    "    # On ajoute niter dans les arguments\n",
    "    def __init__(self, data, n_factors, prior, niter, **kwargs):\n",
    "        \n",
    "        # CORRECTION CRITIQUE : \n",
    "        # On passe niter et prior à la classe parente GenericGibbs\n",
    "        super().__init__(niter=niter, prior=prior, **kwargs)\n",
    "        \n",
    "        self.data = data\n",
    "        self.n = data.shape[1]\n",
    "        self.T = data.shape[0]\n",
    "        self.p = n_factors\n",
    "        \n",
    "        # Le reste de votre __init__ reste identique...\n",
    "        self.theta = prior.sample_initial() \n",
    "        self.z = np.random.normal(size=(self.T, self.p))\n",
    "        self.zeta = np.ones((self.T, self.n))            \n",
    "        self.lambdas = np.zeros((self.T, self.n, self.p)) \n",
    "        self.latent_states = np.zeros((self.T, self.n, self.p))\n",
    "        \n",
    "        for i in range(self.p):\n",
    "            self.lambdas[:, i, i] = 0.1\n",
    "            self.latent_states[:, i, i] = np.log(0.1)\n",
    "\n",
    "        self.ssm_cls = IdentifiedLoadingSSM \n",
    "        self.lambda_path = None\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Une itération complète du Gibbs Sampler (Appendix A)\"\"\"\n",
    "        \n",
    "        # Step 1: Missing Data \n",
    "        # On travaille avec des données complètes pour cet exemple\n",
    "        \n",
    "        # Step 2: Variables de mélange (Zeta)\n",
    "        self.update_zeta()\n",
    "        \n",
    "        # Step 3: Degrees of Freedom (nu)\n",
    "        self.update_nu()\n",
    "        \n",
    "        # Step 4: Facteurs Communs (z_t)\n",
    "        self.update_z()\n",
    "        \n",
    "        # Step 5: State Variables (Lambda)\n",
    "        self.update_lambda_pg()\n",
    "        \n",
    "        # Step 6: Bruit VAR (Sigma) \n",
    "        self.update_sigma()\n",
    "        \n",
    "        # Step 7: Paramètres VAR (mu, Phi)\n",
    "        self.update_mu_phi()\n",
    "        \n",
    "    def update_zeta(self):\n",
    "        \"\"\"\n",
    "        Step 2: Independence Metropolis-Hastings pour zeta.\n",
    "        \n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    def update_nu(self):\n",
    "        \"\"\"\n",
    "        Step 3: Random-walk Metropolis pour nu.\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update_z(self):\n",
    "        \"\"\"\n",
    "        Step 4: Gibbs standard pour z_t.\n",
    "        Mise à jour des facteurs latents communs.\n",
    "        \n",
    "        Sources:\n",
    "        - Distribution conditionnelle z_t ~ N(Mean, Var)\n",
    "        - Calcul de sigma_it et lambda_tilde_it\n",
    "        - Structure de l'inverse de la matrice de corrélation\n",
    "        \"\"\"\n",
    "        T, n = self.data.shape\n",
    "        p = self.p  # Nombre de facteurs\n",
    "        \n",
    "        # 1. Préparation des conteneurs\n",
    "        new_z = np.zeros((T, p))\n",
    "        I_p = np.eye(p)\n",
    "        \n",
    "        # On boucle sur le temps car les paramètres Lambda changent à chaque t\n",
    "        for t in range(T):\n",
    "            # --- A. Récupération des variables au temps t ---\n",
    "            # Données x_it (imputées/transformées)\n",
    "            x_t = self.data[t]  # (n,)\n",
    "            \n",
    "            # Chargements latents lambda_it \n",
    "            # self.lambdas est de forme (T, n, p)\n",
    "            lambda_t = self.lambdas[t] # (n, p)\n",
    "            \n",
    "            # Variables de mélange zeta_t (pour Student-t/Grouped-t)\n",
    "            # Si le modèle est Gaussien, zeta_t = 1 partout.\n",
    "            zeta_t = self.zeta[t] # (n,)\n",
    "\n",
    "            # --- B. Calcul des paramètres redimensionnés (Scaling) ---\n",
    "            # Selon , les paramètres utilisés dans la copule \n",
    "            # (tilde_lambda et sigma) dépendent de l'état latent lambda.\n",
    "            \n",
    "            # Calcul de la norme au carré de lambda pour chaque série i\n",
    "            # lambda_sq = sum(lambda_{it}^2)\n",
    "            lam_sq_norm = np.sum(lambda_t**2, axis=1) # (n,)\n",
    "            \n",
    "            # Variance idiosyncratique sigma_{it}^2 \n",
    "            # sigma^2 = 1 / (1 + lambda'lambda)\n",
    "            sigma_sq_t = 1.0 / (1.0 + lam_sq_norm) # (n,)\n",
    "            sigma_t = np.sqrt(sigma_sq_t)          # (n,)\n",
    "            \n",
    "            # Chargements redimensionnés tilde_lambda \n",
    "            # tilde_lambda = lambda / sqrt(1 + lambda'lambda) = lambda * sigma\n",
    "            lambda_tilde_t = lambda_t * sigma_t[:, np.newaxis] # (n, p)\n",
    "            \n",
    "            # --- C. Construction de la \"Donnée Transformée\" ---\n",
    "            # Selon Step 4 : x_dot = x / sqrt(zeta)\n",
    "            # Cela normalise la variance induite par la variable de mélange\n",
    "            x_dot_t = x_t / np.sqrt(zeta_t) # (n,)\n",
    "\n",
    "            # --- D. Calcul de la Moyenne et Variance Postérieure ---\n",
    "            # Le prior sur z_t est N(0, I_p).\n",
    "            # La vraisemblance est x_dot ~ N(tilde_lambda * z, D)\n",
    "            # où D est diagonale avec éléments sigma_sq_t.\n",
    "            \n",
    "            # Precision Matrix = I_p + C' D^-1 C\n",
    "            # Ici C = lambda_tilde. D^-1 = diag(1/sigma^2).\n",
    "            \n",
    "            # Astuce numérique : tilde_lambda' D^-1 tilde_lambda\n",
    "            # revient à : lambda' lambda (car tilde_lambda = lambda * sigma)\n",
    "            # Preuve: (lambda*sigma)' * (1/sigma^2) * (lambda*sigma) = lambda' * lambda\n",
    "            \n",
    "            # Calcul de la matrice de précision du likelihood\n",
    "            # precision_data = lambda_t.T @ lambda_t # Ce serait l'astuce\n",
    "            # Mais restons fidèles à la notation de l'article pour la clarté :\n",
    "            \n",
    "            # D_inv est un vecteur (diagonale inverse)\n",
    "            D_inv_diag = 1.0 / sigma_sq_t # (n,)\n",
    "            \n",
    "            # Terme C' D^-1\n",
    "            # On multiplie chaque colonne de lambda_tilde par D_inv\n",
    "            Ct_Dinv = lambda_tilde_t.T * D_inv_diag[np.newaxis, :] # (p, n)\n",
    "            \n",
    "            # Precision Postérieure = I + C' D^-1 C\n",
    "            # terme entre crochets\n",
    "            precision_post = I_p + Ct_Dinv @ lambda_tilde_t # (p, p)\n",
    "            \n",
    "            # Covariance Postérieure (Sigma_z)\n",
    "            # On utilise Cholesky pour la stabilité numérique et pour le tirage ensuite\n",
    "            # L = cholesky(Precision) -> Precision = L L.T\n",
    "            try:\n",
    "                L_prec = linalg.cholesky(precision_post, lower=True)\n",
    "                # Pour inverser, on résout le système linéaire. \n",
    "                # Cov = Prec^-1.\n",
    "                # Mais on a juste besoin de résoudre Mean = Cov * Terme_Lineaire\n",
    "                # => Prec * Mean = Terme_Lineaire\n",
    "            except linalg.LinAlgError:\n",
    "                # Fallback en cas de problèmes numériques rares\n",
    "                L_prec = linalg.cholesky(precision_post + 1e-6 * np.eye(p), lower=True)\n",
    "\n",
    "            # Moyenne Postérieure (Mu_z)\n",
    "            # Mean = Cov_post * (C' D^-1 x_dot)\n",
    "            # terme de droite dans la moyenne\n",
    "            linear_term = Ct_Dinv @ x_dot_t # (p,)\n",
    "            \n",
    "            # Résolution de : Precision * Mean = linear_term\n",
    "            # On utilise cholesky_solve pour la rapidité\n",
    "            mu_post = linalg.cho_solve((L_prec, True), linear_term)\n",
    "            \n",
    "            # --- E. Tirage aléatoire (Draw) ---\n",
    "            # z_t = mu_post + chol(Cov_post) * epsilon\n",
    "            # Note: chol(Cov) = inv(L_prec.T)\n",
    "            \n",
    "            epsilon = np.random.normal(size=p)\n",
    "            \n",
    "            # On résout L_prec.T * z_noise = epsilon pour obtenir z_noise ~ N(0, Cov)\n",
    "            # C'est plus stable que d'inverser explicitement\n",
    "            z_noise = linalg.solve_triangular(L_prec.T, epsilon, lower=False)\n",
    "            \n",
    "            new_z[t] = mu_post + z_noise\n",
    "\n",
    "        # Mise à jour de l'attribut de la classe\n",
    "        self.z = new_z\n",
    "\n",
    "    def update_lambda_pg(self):\n",
    "            \"\"\"\n",
    "            Step 5: Mise à jour des chargements factoriels via Particle Gibbs.\n",
    "            \n",
    "            Gère deux ensembles de variables :\n",
    "            1. self.latent_states (h_t) : Variables Gaussiennes suivant l'AR(1).\n",
    "            2. self.lambdas (lambda_t) : Chargements physiques avec contraintes (exp sur diagonale).\n",
    "            \n",
    "            Sources:\n",
    "            - [cite: 302] Utilisation du Particle Gibbs pour échantillonner les trajectoires.\n",
    "            - [cite: 309] Parallélisation possible sur 'i' (ici boucle séquentielle).\n",
    "            - [cite: 343] Utilisation du Backward Sampling pour améliorer le mélange.\n",
    "            -  Application des contraintes d'identification (Triangulaire + Diagonale Log-Normale).\n",
    "            \"\"\"\n",
    "            T, n = self.data.shape\n",
    "            p = self.p\n",
    "            \n",
    "            # A. Préparation des conteneurs (T, n, p)\n",
    "            # Remplis de zéros par défaut (ce qui gère implicitement la partie triangulaire supérieure = 0)\n",
    "            new_lambdas = np.zeros((T, n, p)) \n",
    "            new_states = np.zeros((T, n, p))\n",
    "            \n",
    "            # B. Récupération des paramètres (Broadcasting implicite géré par l'indexation [i])\n",
    "            # On suppose que theta contient des arrays (n, p)\n",
    "            mu_vec = self.theta['mu']      \n",
    "            phi_vec = self.theta['phi']    \n",
    "            sigma_vec = self.theta['sigma'] \n",
    "            \n",
    "            # C. Boucle sur chaque actif i (Indépendance conditionnelle aux facteurs Z)\n",
    "            for i in range(n):\n",
    "                \n",
    "                # 1. Données\n",
    "                # Idéalement, utilisez ici self.transform_u_to_x() si implémenté, \n",
    "                # sinon on prend les données brutes stockées dans self.data\n",
    "                data_i = self.data[:, i] \n",
    "                \n",
    "                # 2. Instanciation du modèle SSM pour l'actif i\n",
    "                # La classe IdentifiedLoadingSSM gère la dimension effective (dim_state)\n",
    "                ssm_i = self.ssm_cls(\n",
    "                    row_idx=i,\n",
    "                    p_factors=p,\n",
    "                    mu=mu_vec[i],        # Vecteur complet (n, p), le SSM prendra ce qu'il faut\n",
    "                    phi=phi_vec[i],\n",
    "                    sigma_eta=sigma_vec[i],\n",
    "                    z=self.z,            # Facteurs communs conditionnels (T, p)\n",
    "                    zeta=self.zeta[:, i], # Variable de mélange (T,)\n",
    "                    data_x=data_i\n",
    "                )\n",
    "                \n",
    "                # 3. Exécution du Particle Gibbs (SMC)\n",
    "                # N=100 particules est suffisant grâce au Backward Sampling [cite: 345]\n",
    "                fk_model = ssm.Bootstrap(ssm=ssm_i, data=data_i)\n",
    "                pf = particles.SMC(fk=fk_model, N=100, store_history=True)\n",
    "                pf.run()\n",
    "                \n",
    "                # 4. Backward Sampling (FFBSm)\n",
    "                # Tire une trajectoire unique h_{1:T} depuis l'historique des particules\n",
    "                # [cite: 334, 343] \"Draw a path ... using backwards sampling\"\n",
    "                traj_list = pf.hist.backward_sampling(1)\n",
    "                \n",
    "                # Conversion liste -> array numpy (T, dim_state)\n",
    "                # [:, :, 0] car backward_sampling renvoie une liste de arrays (M, dim) où M=1\n",
    "                traj_arr = np.array(traj_list)[:, :, 0] \n",
    "                \n",
    "                # 5. Stockage et Transformations\n",
    "                dim = ssm_i.dim_state\n",
    "                \n",
    "                # --- Stockage des États Latents (Gaussiens) ---\n",
    "                # Utilisés pour les étapes 6 & 7 (Update Mu, Phi, Sigma)\n",
    "                new_states[:, i, :dim] = traj_arr\n",
    "                \n",
    "                # --- Stockage des Chargements Physiques ---\n",
    "                # Utilisés pour l'étape 4 (Update Z) et le calcul de vraisemblance\n",
    "                \n",
    "                # Par défaut, on copie les valeurs\n",
    "                new_lambdas[:, i, :dim] = traj_arr\n",
    "                \n",
    "                # Application de la contrainte d'identification pour les p premiers actifs\n",
    "                if i < p:\n",
    "                    # La diagonale (indice i) est modélisée en Log pour garantir la positivité\n",
    "                    # h_{ii} = log(lambda_{ii}) => lambda_{ii} = exp(h_{ii})\n",
    "                    # [cite: 8] \"performed in logarithms to guarantee positive values\"\n",
    "                    new_lambdas[:, i, i] = np.exp(traj_arr[:, i])\n",
    "                    \n",
    "                    # Note: Les éléments new_lambdas[:, i, dim:] restent à 0.0 (Triangulaire inf)\n",
    "\n",
    "            # D. Mise à jour finale des attributs de la classe\n",
    "            self.lambdas = new_lambdas       \n",
    "            self.latent_states = new_states\n",
    "\n",
    "    def update_sigma(self):\n",
    "        \"\"\"\n",
    "        Step 6: Tirage de la variance d'état Sigma (Inverse Gamma).\n",
    "        \n",
    "        Met à jour la variance des chocs de transition des états latents h_t.\n",
    "        Utilise self.latent_states (Gaussian) et NON self.lambdas.\n",
    "        \n",
    "        Sources:\n",
    "        - [cite: 25] Draw Sigma conditional on state variables Lambda.\n",
    "        - [cite: 26] Full conditional posteriors are inverse gamma distributions.\n",
    "        - [cite: 363] Prior utilisé: InvGamma(20, 0.25).\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Récupération des États Latents (h_t)\n",
    "        # C'est ici le changement critique : on utilise les états gaussiens\n",
    "        states_next = self.latent_states[1:]  # h_{2:T} (T-1, n, p)\n",
    "        states_curr = self.latent_states[:-1] # h_{1:T-1} (T-1, n, p)\n",
    "        \n",
    "        # 2. Préparation des paramètres (Broadcasting temporel)\n",
    "        # mu et phi sont (n, p) -> deviennent (1, n, p)\n",
    "        mu = self.theta['mu'][np.newaxis, :, :]\n",
    "        phi = self.theta['phi'][np.newaxis, :, :]\n",
    "        \n",
    "        # 3. Calcul des résidus de la transition AR(1)\n",
    "        # Equation (2): h_{t+1} = mu + Phi(h_t - mu) + eta_t\n",
    "        states_pred = mu + phi * (states_curr - mu)\n",
    "        \n",
    "        # eta_t ~ N(0, Sigma)\n",
    "        residuals = states_next - states_pred # (T-1, n, p)\n",
    "        \n",
    "        # 4. Calcul de la Somme des Carrés des Erreurs (SSE)\n",
    "        # On condense la dimension temporelle (axis=0)\n",
    "        sse = np.sum(residuals**2, axis=0) # (n, p)\n",
    "        \n",
    "        # 5. Paramètres du Postérieur Inverse Gamma\n",
    "        # Prior [cite: 363]\n",
    "        alpha_prior = 20.0\n",
    "        beta_prior = 0.25\n",
    "        \n",
    "        T_eff = self.latent_states.shape[0] - 1\n",
    "        \n",
    "        # Règle de mise à jour conjuguée\n",
    "        alpha_post = alpha_prior + (T_eff / 2.0)\n",
    "        beta_post = beta_prior + (sse / 2.0)\n",
    "        \n",
    "        # 6. Échantillonnage\n",
    "        # Astuce NumPy: X ~ Gamma(a, scale=1/b) => 1/X ~ InvGamma(a, b)\n",
    "        gamma_draws = np.random.gamma(shape=alpha_post, scale=1.0/beta_post)\n",
    "        \n",
    "        # On évite la division par zéro (très improbable mais bonne pratique)\n",
    "        new_sigma = 1.0 / np.maximum(gamma_draws, 1e-10)\n",
    "        \n",
    "        # Mise à jour\n",
    "        self.theta['sigma'] = new_sigma\n",
    "\n",
    "    def update_mu_phi(self):\n",
    "        \"\"\"\n",
    "        Step 7: Tirage de mu et Phi conditionnellement aux états latents.\n",
    "        \n",
    "        Effectue une régression bayésienne sur les états latents h_t (self.latent_states).\n",
    "        Gère la contrainte de stationnarité pour Phi via une Normale Tronquée.\n",
    "        \n",
    "        Sources:\n",
    "        - [cite: 27] Draw mu, Phi conditional on state variables... acceptance sampling.\n",
    "        -[cite: 164]...performed in logarithms (géré par l'usage de latent_states).\n",
    "        - [cite: 362] Prior mu ~ N(0.4, 2).\n",
    "        - [cite: 363] Prior Phi ~ N(0.985, 0.001) tronqué sur (-1, 1).\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Récupération des États Latents (Gaussian h_t)\n",
    "        # Changement critique: utilisation de latent_states\n",
    "        h_curr = self.latent_states[:-1] # (T-1, n, p)\n",
    "        h_next = self.latent_states[1:]  # (T-1, n, p)\n",
    "        T_eff = h_curr.shape[0]\n",
    "        \n",
    "        # Variance actuelle sigma^2 (n, p)\n",
    "        sigma_sq = self.theta['sigma'] \n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # A. Mise à jour de MU (Moyenne Long Terme)\n",
    "        # Modèle: (h_{t+1} - Phi*h_t) = (1 - Phi)*mu + eta_t\n",
    "        # -------------------------------------------------------\n",
    "        phi = self.theta['phi'] # (n, p)\n",
    "        \n",
    "        # Prior Mu [cite: 362]\n",
    "        mu_prior_mean = 0.4\n",
    "        mu_prior_var = 2.0\n",
    "        mu_prior_prec = 1.0 / mu_prior_var\n",
    "        \n",
    "        # Pseudo-Variable dépendante Y\n",
    "        y_mu = h_next - phi[np.newaxis, :, :] * h_curr # (T-1, n, p)\n",
    "        \n",
    "        # Regresseur constant X = (1 - Phi)\n",
    "        X_mu = (1.0 - phi) # (n, p)\n",
    "        \n",
    "        # Précision et Moyenne pondérée (Likelihood terms)\n",
    "        # Somme sur T des précisions: T * X^2 / sigma^2\n",
    "        lik_prec_mu = T_eff * (X_mu**2) / sigma_sq\n",
    "        \n",
    "        # Somme sur T des (X * Y / sigma^2)\n",
    "        lik_mean_term_mu = (X_mu / sigma_sq) * np.sum(y_mu, axis=0)\n",
    "        \n",
    "        # Postérieur Mu\n",
    "        post_prec_mu = mu_prior_prec + lik_prec_mu\n",
    "        post_var_mu = 1.0 / post_prec_mu\n",
    "        post_mean_mu = post_var_mu * (mu_prior_prec * mu_prior_mean + lik_mean_term_mu)\n",
    "        \n",
    "        # Tirage Normal\n",
    "        self.theta['mu'] = np.random.normal(loc=post_mean_mu, scale=np.sqrt(post_var_mu))\n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # B. Mise à jour de PHI (Autocorrélation)\n",
    "        # Modèle: (h_{t+1} - mu) = Phi * (h_t - mu) + eta_t\n",
    "        # -------------------------------------------------------\n",
    "        mu = self.theta['mu'] # (n, p) (Nouvelle valeur)\n",
    "        \n",
    "        # Prior Phi [cite: 363]\n",
    "        phi_prior_mean = 0.985\n",
    "        phi_prior_var = 0.001\n",
    "        phi_prior_prec = 1.0 / phi_prior_var\n",
    "        \n",
    "        # Centrage des données (X = h_t - mu)\n",
    "        x_phi = h_curr - mu[np.newaxis, :, :]\n",
    "        y_phi = h_next - mu[np.newaxis, :, :]\n",
    "        \n",
    "        # Statistiques suffisantes (Sommes des produits croisés)\n",
    "        sum_x_sq = np.sum(x_phi**2, axis=0)      # sum(X^2)\n",
    "        sum_xy = np.sum(x_phi * y_phi, axis=0)   # sum(X*Y)\n",
    "        \n",
    "        # Termes de vraisemblance\n",
    "        lik_prec_phi = sum_x_sq / sigma_sq\n",
    "        lik_mean_term_phi = sum_xy / sigma_sq\n",
    "        \n",
    "        # Postérieur Phi (Non Tronqué)\n",
    "        post_prec_phi = phi_prior_prec + lik_prec_phi\n",
    "        post_var_phi = 1.0 / post_prec_phi\n",
    "        post_mean_phi = post_var_phi * (phi_prior_prec * phi_prior_mean + lik_mean_term_phi)\n",
    "        post_std_phi = np.sqrt(post_var_phi)\n",
    "        \n",
    "        # Tirage Tronqué sur (-1, 1) pour stationnarité\n",
    "        # Calcul des bornes normalisées pour truncnorm (a, b)\n",
    "        # Z = (X - mean) / std\n",
    "        a_clip = (-1.0 - post_mean_phi) / post_std_phi\n",
    "        b_clip = (1.0 - post_mean_phi) / post_std_phi\n",
    "        \n",
    "        # Utilisation de scipy.stats.truncnorm (vectorisé)\n",
    "        self.theta['phi'] = stats.truncnorm.rvs(\n",
    "            a_clip, \n",
    "            b_clip, \n",
    "            loc=post_mean_phi, \n",
    "            scale=post_std_phi, \n",
    "            size=mu.shape\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57260391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation de données synthétiques : T=100, n=5, p=2\n",
      "Initialisation du Gibbs Sampler...\n",
      "Lancement du Gibbs...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FactorCopulaPrior' object has no attribute 'rvs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 141\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# 4. Lancer\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLancement du Gibbs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 141\u001b[0m \u001b[43mgibbs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Plus besoin de passer niter ici, il est déjà dans l'objet\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTerminé avec succès !\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vautr\\anaconda3\\envs\\env_qrt_foot\\Lib\\site-packages\\particles\\utils.py:85\u001b[0m, in \u001b[0;36mtimer.<locals>.timed_method\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtimed_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     84\u001b[0m     starting_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m---> 85\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m starting_time\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\vautr\\anaconda3\\envs\\env_qrt_foot\\Lib\\site-packages\\particles\\mcmc.py:177\u001b[0m, in \u001b[0;36mMCMC.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mniter):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 177\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep0\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(n)\n",
      "File \u001b[1;32mc:\\Users\\vautr\\anaconda3\\envs\\env_qrt_foot\\Lib\\site-packages\\particles\\mcmc.py:520\u001b[0m, in \u001b[0;36mGenericGibbs.step0\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep0\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 520\u001b[0m     th0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprior\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrvs\u001b[49m(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta0 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta0\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mtheta[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m th0\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_states(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mtheta[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FactorCopulaPrior' object has no attribute 'rvs'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Classe Prior (Indispensable pour l'initialisation)\n",
    "# ---------------------------------------------------------\n",
    "class FactorCopulaPrior:\n",
    "    def __init__(self, n, p):\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "        self.dtype = [\n",
    "            ('mu', 'float64', (n, p)),\n",
    "            ('phi', 'float64', (n, p)),\n",
    "            ('sigma', 'float64', (n, p))\n",
    "        ]\n",
    "\n",
    "    def sample_initial(self):\n",
    "        \"\"\"Initialise les paramètres aléatoirement pour le démarrage du Gibbs\"\"\"\n",
    "        # Mu ~ N(0, 1) pour l'exemple\n",
    "        mu = np.random.normal(0, 0.5, size=(self.n, self.p))\n",
    "        \n",
    "        # Phi ~ Uniform(0.5, 0.9) pour être stable\n",
    "        phi = np.random.uniform(0.5, 0.9, size=(self.n, self.p))\n",
    "        \n",
    "        # Sigma ~ Petite variance pour commencer\n",
    "        sigma = np.ones((self.n, self.p)) * 0.1\n",
    "        \n",
    "        return {\n",
    "            'mu': mu,\n",
    "            'phi': phi,\n",
    "            'sigma': sigma\n",
    "        }\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Fonction pour générer les fausses données (Truth)\n",
    "# ---------------------------------------------------------\n",
    "def simulate_data(T, n, p):\n",
    "    print(f\"Simulation de données synthétiques : T={T}, n={n}, p={p}\")\n",
    "    \n",
    "    # --- A. Vrais Paramètres ---\n",
    "    true_mu = np.ones((n, p)) * 0.5\n",
    "    true_phi = np.ones((n, p)) * 0.9  # Persistant\n",
    "    true_sigma = np.ones((n, p)) * 0.1\n",
    "    \n",
    "    # --- B. Simulation des Facteurs Communs (z_t) ---\n",
    "    # z_t ~ N(0, I)\n",
    "    z = np.random.normal(size=(T, p))\n",
    "    \n",
    "    # --- C. Simulation des Chargements Stochastiques (Lambda_t) ---\n",
    "    # h_t = mu + phi(h_{t-1} - mu) + eta\n",
    "    lambdas = np.zeros((T, n, p))\n",
    "    h = np.zeros((T, n, p)) # États latents\n",
    "    \n",
    "    # Initialisation h_0 = mu\n",
    "    h[0] = true_mu\n",
    "    \n",
    "    # Boucle temporelle pour AR(1)\n",
    "    for t in range(1, T):\n",
    "        eta = np.random.normal(0, np.sqrt(true_sigma))\n",
    "        h[t] = true_mu + true_phi * (h[t-1] - true_mu) + eta\n",
    "\n",
    "    # Transformation Log -> Exp pour la diagonale (Identification)\n",
    "    # Copie les états vers les chargements\n",
    "    lambdas = h.copy()\n",
    "    for i in range(n):\n",
    "        if i < p:\n",
    "            # Diagonale positive\n",
    "            lambdas[:, i, i] = np.exp(h[:, i, i])\n",
    "            # Triangulaire inférieure (force 0 au dessus)\n",
    "            lambdas[:, i, i+1:] = 0.0\n",
    "\n",
    "    # --- D. Simulation des Observations (x_it) ---\n",
    "    # x_it = (lambda_it * z_t) + epsilon_it\n",
    "    # Note : Dans le modèle complet, il y a un scaling sigma_eps\n",
    "    # Scaling factor D_it = sqrt(1 + lambda'lambda)\n",
    "    \n",
    "    x = np.zeros((T, n))\n",
    "    zeta = np.ones((T, n)) # On suppose Gaussien pur pour le test (zeta=1)\n",
    "    \n",
    "    for t in range(T):\n",
    "        for i in range(n):\n",
    "            lam = lambdas[t, i] # (p,)\n",
    "            \n",
    "            # Produit scalaire facteur * loading\n",
    "            factor_part = np.dot(lam, z[t])\n",
    "            \n",
    "            # Variance idiosyncratique (Scaling de l'article)\n",
    "            # sigma_eps = 1 / sqrt(1 + lambda^2)\n",
    "            lam_sq = np.sum(lam**2)\n",
    "            scaling = np.sqrt(1 + lam_sq)\n",
    "            sigma_eps = 1.0 / scaling\n",
    "            \n",
    "            # lambda_tilde = lambda / scaling\n",
    "            \n",
    "            # Le modèle génératif implicite de l'observation normalisée :\n",
    "            # x_raw ~ N(0, 1) transformé par la corrélation\n",
    "            # Ici on simule directement x selon l'eq (5) simplifiée\n",
    "            # x_it = sqrt(zeta) * (tilde_lambda * z + sigma_eps * eps)\n",
    "            \n",
    "            eps = np.random.normal(0, 1)\n",
    "            lambda_tilde = lam / scaling\n",
    "            \n",
    "            mean_x = np.sqrt(zeta[t, i]) * np.dot(lambda_tilde, z[t])\n",
    "            std_x = np.sqrt(zeta[t, i]) * sigma_eps\n",
    "            \n",
    "            x[t, i] = np.random.normal(mean_x, std_x)\n",
    "            \n",
    "    return x\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Script de Test Corrigé\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Paramètres de test\n",
    "T_sim = 100\n",
    "n_sim = 5\n",
    "p_sim = 2\n",
    "\n",
    "# 1. Générer les données\n",
    "data_x = simulate_data(T_sim, n_sim, p_sim)\n",
    "\n",
    "# 2. Créer le Prior\n",
    "my_prior = FactorCopulaPrior(n=n_sim, p=p_sim)\n",
    "\n",
    "# 3. Instancier le Gibbs\n",
    "print(\"Initialisation du Gibbs Sampler...\")\n",
    "gibbs = FactorCopulaGibbs(\n",
    "    data=data_x, \n",
    "    n_factors=p_sim, \n",
    "    prior=my_prior,\n",
    "    niter=20  # Utiliser 'niter' et non 'n_iter'\n",
    ")\n",
    "\n",
    "# Petit hack pour la méthode transform\n",
    "def dummy_transform():\n",
    "    return gibbs.data \n",
    "gibbs.transform_u_to_x = dummy_transform\n",
    "\n",
    "# 4. Lancer\n",
    "print(\"Lancement du Gibbs...\")\n",
    "gibbs.run() # Plus besoin de passer niter ici, il est déjà dans l'objet\n",
    "\n",
    "print(\"Terminé avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94b623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qrt_foot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
