{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ccccc9",
   "metadata": {},
   "source": [
    "# Implementation of the Gibbs sampler from the section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74c247f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from particles import state_space_models as ssm\n",
    "from particles import distributions as dists\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import particles\n",
    "from particles import distributions\n",
    "from scipy import stats\n",
    "from particles import mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fca90b",
   "metadata": {},
   "source": [
    "On creer notre state space modele lorsque conditionnellement à z et zeta. Deplus ce state space modèle est défini unique pour un actif car conditionnelement à z et zeta nos actif sont independant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8842fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentifiedLoadingSSM(ssm.StateSpaceModel):\n",
    "    \"\"\"\n",
    "    Modèle espace d'état pour les chargements d'un actif 'i' avec contraintes d'identification.\n",
    "    \n",
    "    Règles d'identification :\n",
    "    - Pour les p premières lignes (i < p):\n",
    "        * La matrice est triangulaire inférieure (lambda_{ik} = 0 pour k > i).\n",
    "        * Les éléments diagonaux (lambda_{ii}) sont stricts positifs -> modélisés en log.\n",
    "    - Pour les lignes suivantes (i >= p):\n",
    "        * Pas de contrainte de structure ou de signe.\n",
    "        \n",
    "    Attributs:\n",
    "        row_idx (int): Indice de l'actif courant (i).\n",
    "        p_factors (int): Nombre total de facteurs.\n",
    "        dim_state (int): Dimension effective du vecteur d'état pour cet actif.\n",
    "    \"\"\"\n",
    "    def __init__(self, row_idx, p_factors, mu, phi, sigma_eta, z, zeta, data_x):\n",
    "        self.i = row_idx\n",
    "        self.p = p_factors\n",
    "        \n",
    "        # Dimension effective: Pour i < p, on ne simule que les i+1 premiers éléments.\n",
    "        self.dim_state = min(row_idx + 1, p_factors)\n",
    "        \n",
    "        # On ne conserve que les paramètres pertinents pour la dimension active\n",
    "        self.mu = mu[:self.dim_state]         # (dim_state,)\n",
    "        self.phi = phi[:self.dim_state]       # (dim_state,)\n",
    "        self.sigma_eta = sigma_eta[:self.dim_state] # (dim_state,)\n",
    "        \n",
    "        self.z = z       # Facteurs communs (T, p)\n",
    "        self.zeta = zeta # Variable de mélange (T,)\n",
    "        self.data_x = data_x # Données observées (T,)\n",
    "\n",
    "    def PX0(self):\n",
    "        \"\"\"Distribution initiale des états h_0 \"\"\"\n",
    "        # Variance stationnaire: sigma^2 / (1 - phi^2)\n",
    "        var_0 = self.sigma_eta**2 / (1 - self.phi**2)\n",
    "        \n",
    "        # Distribution indépendante sur chaque composante du vecteur d'état\n",
    "        return distributions.IndepProd(\n",
    "            distributions.Normal(loc=self.mu, scale=np.sqrt(var_0))\n",
    "        )\n",
    "\n",
    "    def PX(self, t, xp):\n",
    "        \"\"\"Transition des états h_t | h_{t-1} \"\"\"\n",
    "        # Dynamique AR(1) Gaussienne sur h_t\n",
    "        # Note: Si k=i (diagonale), h_t correspond au LOG-loading.\n",
    "        mean = self.mu + self.phi * (xp - self.mu)\n",
    "        return distributions.IndepProd(\n",
    "            distributions.Normal(loc=mean, scale=self.sigma_eta)\n",
    "        )\n",
    "\n",
    "    def PY(self, t, xp, x):\n",
    "        \"\"\"\n",
    "        Vraisemblance p(x_it | h_it, z_t, zeta_t) \n",
    "        Transforme l'état latent h_t en chargement lambda_t puis calcule la densité.\n",
    "        \"\"\"\n",
    "        # x est de forme (N_particules, dim_state)\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        # 1. Reconstitution du vecteur de chargements complet (taille p)\n",
    "        real_loadings = np.zeros((N, self.p)) \n",
    "        \n",
    "        # Copie des états latents\n",
    "        real_loadings[:, :self.dim_state] = x\n",
    "        \n",
    "        # 2. Application de la contrainte de positivité (Log -> Exp)\n",
    "        # Uniquement pour les éléments diagonaux des p premiers actifs\n",
    "        if self.i < self.p:\n",
    "            # L'élément diagonal est le dernier de l'état actif (indice self.i)\n",
    "            # h_{ii} = log(lambda_{ii})  =>  lambda_{ii} = exp(h_{ii})\n",
    "            real_loadings[:, self.i] = np.exp(x[:, self.i])\n",
    "            \n",
    "            # Les éléments k > i restent à 0.0 (Triangulaire inf)\n",
    "            \n",
    "        # 3. Calcul de la composante factorielle et du scaling\n",
    "        z_t = self.z[t] # (p,)\n",
    "        \n",
    "        # Produit scalaire lambda' * z\n",
    "        factor_comp = np.dot(real_loadings, z_t) # (N,)\n",
    "        \n",
    "        # Norme au carré des lambda pour le scaling sigma_{it} \n",
    "        lam_sq = np.sum(real_loadings**2, axis=1) # (N,)\n",
    "        \n",
    "        # Scaling factor: sqrt(1 + lambda'lambda)\n",
    "        scaling = np.sqrt(1.0 + lam_sq)\n",
    "        \n",
    "        # Écart-type résiduel: sigma = 1 / scaling\n",
    "        sigma_eps = 1.0 / scaling\n",
    "        \n",
    "        # 4. Paramètres de la loi Normale de l'observation x_it\n",
    "        # x_it = sqrt(zeta) * ( (lambda/scaling)'z + (1/scaling)*eps )\n",
    "        zeta_sqrt = np.sqrt(self.zeta[t])\n",
    "        \n",
    "        mean_obs = zeta_sqrt * (factor_comp / scaling)\n",
    "        scale_obs = zeta_sqrt * sigma_eps\n",
    "        \n",
    "        return distributions.Normal(loc=mean_obs, scale=scale_obs).logpdf(self.data_x[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a676f8",
   "metadata": {},
   "source": [
    "On ne pourra pas directement utilisé le mcmc.ParticleGibbs de particules car l'un des principales aventage de l'augmented Gibbs est que conditionnelement au facteur on peut faire chaque actif est indépendant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorCopulaGibbs(mcmc.GenericGibbs):\n",
    "    \"\"\"\n",
    "    Échantillonneur de Gibbs pour le modèle de Copule Factorielle Dynamique.\n",
    "    Basé sur l'Appendix A de Creal & Tsay (2015).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, n_factors, prior, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data = data # u_it\n",
    "        self.n = data.shape[1]\n",
    "        self.T = data.shape[0]\n",
    "        self.p = n_factors\n",
    "        \n",
    "        # Initialisation des paramètres Theta \n",
    "        self.theta = prior.sample_initial() \n",
    "        # Contient: mu, phi, sigma, nu (degrees of freedom), beta\n",
    "        \n",
    "        # Initialisation des variables latentes \n",
    "        self.z = np.random.normal(size=(self.T, self.p)) # Facteurs communs\n",
    "        self.zeta = np.ones((self.T, self.n))            # Variables de mélange\n",
    "\n",
    "        # A. Les Chargements Physiques (Lambdas)\n",
    "        # Ce sont les valeurs réelles utilisées dans l'équation d'observation (5).\n",
    "        # Pour les éléments diagonaux, ce sont des valeurs strictement POSITIVES.\n",
    "        self.lambdas = np.zeros((self.T, self.n, self.p)) \n",
    "        \n",
    "        # B. Les États Latents Gaussiens (States)\n",
    "        # Ce sont les variables h_t qui suivent la dynamique AR(1) de l'équation (2).\n",
    "        # Pour la diagonale : latent_states = log(lambdas)\n",
    "        # Pour le reste : latent_states = lambdas\n",
    "        self.latent_states = np.zeros((self.T, self.n, self.p))\n",
    "        \n",
    "        # Initialisation cohérente (optionnelle mais recommandée pour éviter log(0))\n",
    "        # On peut initialiser la diagonale à une petite valeur positive pour lambda\n",
    "        for i in range(self.p):\n",
    "            # Diagonale initiale lambda = 0.1 -> latent = log(0.1)\n",
    "            self.lambdas[:, i, i] = 0.1\n",
    "            self.latent_states[:, i, i] = np.log(0.1)\n",
    "\n",
    "        # 5. Attributs pour le Particle Gibbs\n",
    "        self.ssm_cls = IdentifiedLoadingSSM # On utilise la bonne classe identifiée\n",
    "        self.lambda_path = None\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Une itération complète du Gibbs Sampler (Appendix A)\"\"\"\n",
    "        \n",
    "        # Step 1: Missing Data \n",
    "        # On travaille avec des données complètes pour cet exemple\n",
    "        \n",
    "        # Step 2: Variables de mélange (Zeta)\n",
    "        self.update_zeta()\n",
    "        \n",
    "        # Step 3: Degrees of Freedom (nu)\n",
    "        self.update_nu()\n",
    "        \n",
    "        # Step 4: Facteurs Communs (z_t)\n",
    "        self.update_z()\n",
    "        \n",
    "        # Step 5: State Variables (Lambda)\n",
    "        self.update_lambda_pg()\n",
    "        \n",
    "        # Step 6: Bruit VAR (Sigma) \n",
    "        self.update_sigma()\n",
    "        \n",
    "        # Step 7: Paramètres VAR (mu, Phi)\n",
    "        self.update_mu_phi()\n",
    "        \n",
    "    def update_zeta(self):\n",
    "        \"\"\"\n",
    "        Step 2: Independence Metropolis-Hastings pour zeta.\n",
    "        \n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    def update_nu(self):\n",
    "        \"\"\"\n",
    "        Step 3: Random-walk Metropolis pour nu.\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update_z(self):\n",
    "        \"\"\"\n",
    "        Step 4: Gibbs standard pour z_t.\n",
    "        Mise à jour des facteurs latents communs.\n",
    "        \n",
    "        Sources:\n",
    "        - Distribution conditionnelle z_t ~ N(Mean, Var)\n",
    "        - Calcul de sigma_it et lambda_tilde_it\n",
    "        - Structure de l'inverse de la matrice de corrélation\n",
    "        \"\"\"\n",
    "        T, n = self.data.shape\n",
    "        p = self.p  # Nombre de facteurs\n",
    "        \n",
    "        # 1. Préparation des conteneurs\n",
    "        new_z = np.zeros((T, p))\n",
    "        I_p = np.eye(p)\n",
    "        \n",
    "        # On boucle sur le temps car les paramètres Lambda changent à chaque t\n",
    "        for t in range(T):\n",
    "            # --- A. Récupération des variables au temps t ---\n",
    "            # Données x_it (imputées/transformées)\n",
    "            x_t = self.data[t]  # (n,)\n",
    "            \n",
    "            # Chargements latents lambda_it \n",
    "            # self.lambdas est de forme (T, n, p)\n",
    "            lambda_t = self.lambdas[t] # (n, p)\n",
    "            \n",
    "            # Variables de mélange zeta_t (pour Student-t/Grouped-t)\n",
    "            # Si le modèle est Gaussien, zeta_t = 1 partout.\n",
    "            zeta_t = self.zeta[t] # (n,)\n",
    "\n",
    "            # --- B. Calcul des paramètres redimensionnés (Scaling) ---\n",
    "            # Selon , les paramètres utilisés dans la copule \n",
    "            # (tilde_lambda et sigma) dépendent de l'état latent lambda.\n",
    "            \n",
    "            # Calcul de la norme au carré de lambda pour chaque série i\n",
    "            # lambda_sq = sum(lambda_{it}^2)\n",
    "            lam_sq_norm = np.sum(lambda_t**2, axis=1) # (n,)\n",
    "            \n",
    "            # Variance idiosyncratique sigma_{it}^2 \n",
    "            # sigma^2 = 1 / (1 + lambda'lambda)\n",
    "            sigma_sq_t = 1.0 / (1.0 + lam_sq_norm) # (n,)\n",
    "            sigma_t = np.sqrt(sigma_sq_t)          # (n,)\n",
    "            \n",
    "            # Chargements redimensionnés tilde_lambda \n",
    "            # tilde_lambda = lambda / sqrt(1 + lambda'lambda) = lambda * sigma\n",
    "            lambda_tilde_t = lambda_t * sigma_t[:, np.newaxis] # (n, p)\n",
    "            \n",
    "            # --- C. Construction de la \"Donnée Transformée\" ---\n",
    "            # Selon Step 4 : x_dot = x / sqrt(zeta)\n",
    "            # Cela normalise la variance induite par la variable de mélange\n",
    "            x_dot_t = x_t / np.sqrt(zeta_t) # (n,)\n",
    "\n",
    "            # --- D. Calcul de la Moyenne et Variance Postérieure ---\n",
    "            # Le prior sur z_t est N(0, I_p).\n",
    "            # La vraisemblance est x_dot ~ N(tilde_lambda * z, D)\n",
    "            # où D est diagonale avec éléments sigma_sq_t.\n",
    "            \n",
    "            # Precision Matrix = I_p + C' D^-1 C\n",
    "            # Ici C = lambda_tilde. D^-1 = diag(1/sigma^2).\n",
    "            \n",
    "            # Astuce numérique : tilde_lambda' D^-1 tilde_lambda\n",
    "            # revient à : lambda' lambda (car tilde_lambda = lambda * sigma)\n",
    "            # Preuve: (lambda*sigma)' * (1/sigma^2) * (lambda*sigma) = lambda' * lambda\n",
    "            \n",
    "            # Calcul de la matrice de précision du likelihood\n",
    "            # precision_data = lambda_t.T @ lambda_t # Ce serait l'astuce\n",
    "            # Mais restons fidèles à la notation de l'article pour la clarté :\n",
    "            \n",
    "            # D_inv est un vecteur (diagonale inverse)\n",
    "            D_inv_diag = 1.0 / sigma_sq_t # (n,)\n",
    "            \n",
    "            # Terme C' D^-1\n",
    "            # On multiplie chaque colonne de lambda_tilde par D_inv\n",
    "            Ct_Dinv = lambda_tilde_t.T * D_inv_diag[np.newaxis, :] # (p, n)\n",
    "            \n",
    "            # Precision Postérieure = I + C' D^-1 C\n",
    "            # terme entre crochets\n",
    "            precision_post = I_p + Ct_Dinv @ lambda_tilde_t # (p, p)\n",
    "            \n",
    "            # Covariance Postérieure (Sigma_z)\n",
    "            # On utilise Cholesky pour la stabilité numérique et pour le tirage ensuite\n",
    "            # L = cholesky(Precision) -> Precision = L L.T\n",
    "            try:\n",
    "                L_prec = linalg.cholesky(precision_post, lower=True)\n",
    "                # Pour inverser, on résout le système linéaire. \n",
    "                # Cov = Prec^-1.\n",
    "                # Mais on a juste besoin de résoudre Mean = Cov * Terme_Lineaire\n",
    "                # => Prec * Mean = Terme_Lineaire\n",
    "            except linalg.LinAlgError:\n",
    "                # Fallback en cas de problèmes numériques rares\n",
    "                L_prec = linalg.cholesky(precision_post + 1e-6 * np.eye(p), lower=True)\n",
    "\n",
    "            # Moyenne Postérieure (Mu_z)\n",
    "            # Mean = Cov_post * (C' D^-1 x_dot)\n",
    "            # terme de droite dans la moyenne\n",
    "            linear_term = Ct_Dinv @ x_dot_t # (p,)\n",
    "            \n",
    "            # Résolution de : Precision * Mean = linear_term\n",
    "            # On utilise cholesky_solve pour la rapidité\n",
    "            mu_post = linalg.cho_solve((L_prec, True), linear_term)\n",
    "            \n",
    "            # --- E. Tirage aléatoire (Draw) ---\n",
    "            # z_t = mu_post + chol(Cov_post) * epsilon\n",
    "            # Note: chol(Cov) = inv(L_prec.T)\n",
    "            \n",
    "            epsilon = np.random.normal(size=p)\n",
    "            \n",
    "            # On résout L_prec.T * z_noise = epsilon pour obtenir z_noise ~ N(0, Cov)\n",
    "            # C'est plus stable que d'inverser explicitement\n",
    "            z_noise = linalg.solve_triangular(L_prec.T, epsilon, lower=False)\n",
    "            \n",
    "            new_z[t] = mu_post + z_noise\n",
    "\n",
    "        # Mise à jour de l'attribut de la classe\n",
    "        self.z = new_z\n",
    "\n",
    "    def update_lambda_pg(self):\n",
    "            \"\"\"\n",
    "            Step 5: Mise à jour des chargements factoriels via Particle Gibbs.\n",
    "            \n",
    "            Gère deux ensembles de variables :\n",
    "            1. self.latent_states (h_t) : Variables Gaussiennes suivant l'AR(1).\n",
    "            2. self.lambdas (lambda_t) : Chargements physiques avec contraintes (exp sur diagonale).\n",
    "            \n",
    "            Sources:\n",
    "            - [cite: 302] Utilisation du Particle Gibbs pour échantillonner les trajectoires.\n",
    "            - [cite: 309] Parallélisation possible sur 'i' (ici boucle séquentielle).\n",
    "            - [cite: 343] Utilisation du Backward Sampling pour améliorer le mélange.\n",
    "            -  Application des contraintes d'identification (Triangulaire + Diagonale Log-Normale).\n",
    "            \"\"\"\n",
    "            T, n = self.data.shape\n",
    "            p = self.p\n",
    "            \n",
    "            # A. Préparation des conteneurs (T, n, p)\n",
    "            # Remplis de zéros par défaut (ce qui gère implicitement la partie triangulaire supérieure = 0)\n",
    "            new_lambdas = np.zeros((T, n, p)) \n",
    "            new_states = np.zeros((T, n, p))\n",
    "            \n",
    "            # B. Récupération des paramètres (Broadcasting implicite géré par l'indexation [i])\n",
    "            # On suppose que theta contient des arrays (n, p)\n",
    "            mu_vec = self.theta['mu']      \n",
    "            phi_vec = self.theta['phi']    \n",
    "            sigma_vec = self.theta['sigma'] \n",
    "            \n",
    "            # C. Boucle sur chaque actif i (Indépendance conditionnelle aux facteurs Z)\n",
    "            for i in range(n):\n",
    "                \n",
    "                # 1. Données\n",
    "                # Idéalement, utilisez ici self.transform_u_to_x() si implémenté, \n",
    "                # sinon on prend les données brutes stockées dans self.data\n",
    "                data_i = self.data[:, i] \n",
    "                \n",
    "                # 2. Instanciation du modèle SSM pour l'actif i\n",
    "                # La classe IdentifiedLoadingSSM gère la dimension effective (dim_state)\n",
    "                ssm_i = self.ssm_cls(\n",
    "                    row_idx=i,\n",
    "                    p_factors=p,\n",
    "                    mu=mu_vec[i],        # Vecteur complet (n, p), le SSM prendra ce qu'il faut\n",
    "                    phi=phi_vec[i],\n",
    "                    sigma_eta=sigma_vec[i],\n",
    "                    z=self.z,            # Facteurs communs conditionnels (T, p)\n",
    "                    zeta=self.zeta[:, i], # Variable de mélange (T,)\n",
    "                    data_x=data_i\n",
    "                )\n",
    "                \n",
    "                # 3. Exécution du Particle Gibbs (SMC)\n",
    "                # N=100 particules est suffisant grâce au Backward Sampling [cite: 345]\n",
    "                fk_model = ssm.Bootstrap(ssm=ssm_i, data=data_i)\n",
    "                pf = particles.SMC(fk=fk_model, N=100, store_history=True)\n",
    "                pf.run()\n",
    "                \n",
    "                # 4. Backward Sampling (FFBSm)\n",
    "                # Tire une trajectoire unique h_{1:T} depuis l'historique des particules\n",
    "                # [cite: 334, 343] \"Draw a path ... using backwards sampling\"\n",
    "                traj_list = pf.hist.backward_sampling(1)\n",
    "                \n",
    "                # Conversion liste -> array numpy (T, dim_state)\n",
    "                # [:, :, 0] car backward_sampling renvoie une liste de arrays (M, dim) où M=1\n",
    "                traj_arr = np.array(traj_list)[:, :, 0] \n",
    "                \n",
    "                # 5. Stockage et Transformations\n",
    "                dim = ssm_i.dim_state\n",
    "                \n",
    "                # --- Stockage des États Latents (Gaussiens) ---\n",
    "                # Utilisés pour les étapes 6 & 7 (Update Mu, Phi, Sigma)\n",
    "                new_states[:, i, :dim] = traj_arr\n",
    "                \n",
    "                # --- Stockage des Chargements Physiques ---\n",
    "                # Utilisés pour l'étape 4 (Update Z) et le calcul de vraisemblance\n",
    "                \n",
    "                # Par défaut, on copie les valeurs\n",
    "                new_lambdas[:, i, :dim] = traj_arr\n",
    "                \n",
    "                # Application de la contrainte d'identification pour les p premiers actifs\n",
    "                if i < p:\n",
    "                    # La diagonale (indice i) est modélisée en Log pour garantir la positivité\n",
    "                    # h_{ii} = log(lambda_{ii}) => lambda_{ii} = exp(h_{ii})\n",
    "                    # [cite: 8] \"performed in logarithms to guarantee positive values\"\n",
    "                    new_lambdas[:, i, i] = np.exp(traj_arr[:, i])\n",
    "                    \n",
    "                    # Note: Les éléments new_lambdas[:, i, dim:] restent à 0.0 (Triangulaire inf)\n",
    "\n",
    "            # D. Mise à jour finale des attributs de la classe\n",
    "            self.lambdas = new_lambdas       \n",
    "            self.latent_states = new_states\n",
    "\n",
    "    def update_sigma(self):\n",
    "        \"\"\"\n",
    "        Step 6: Tirage de la variance d'état Sigma (Inverse Gamma).\n",
    "        \n",
    "        Met à jour la variance des chocs de transition des états latents h_t.\n",
    "        Utilise self.latent_states (Gaussian) et NON self.lambdas.\n",
    "        \n",
    "        Sources:\n",
    "        - [cite: 25] Draw Sigma conditional on state variables Lambda.\n",
    "        - [cite: 26] Full conditional posteriors are inverse gamma distributions.\n",
    "        - [cite: 363] Prior utilisé: InvGamma(20, 0.25).\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Récupération des États Latents (h_t)\n",
    "        # C'est ici le changement critique : on utilise les états gaussiens\n",
    "        states_next = self.latent_states[1:]  # h_{2:T} (T-1, n, p)\n",
    "        states_curr = self.latent_states[:-1] # h_{1:T-1} (T-1, n, p)\n",
    "        \n",
    "        # 2. Préparation des paramètres (Broadcasting temporel)\n",
    "        # mu et phi sont (n, p) -> deviennent (1, n, p)\n",
    "        mu = self.theta['mu'][np.newaxis, :, :]\n",
    "        phi = self.theta['phi'][np.newaxis, :, :]\n",
    "        \n",
    "        # 3. Calcul des résidus de la transition AR(1)\n",
    "        # Equation (2): h_{t+1} = mu + Phi(h_t - mu) + eta_t\n",
    "        states_pred = mu + phi * (states_curr - mu)\n",
    "        \n",
    "        # eta_t ~ N(0, Sigma)\n",
    "        residuals = states_next - states_pred # (T-1, n, p)\n",
    "        \n",
    "        # 4. Calcul de la Somme des Carrés des Erreurs (SSE)\n",
    "        # On condense la dimension temporelle (axis=0)\n",
    "        sse = np.sum(residuals**2, axis=0) # (n, p)\n",
    "        \n",
    "        # 5. Paramètres du Postérieur Inverse Gamma\n",
    "        # Prior [cite: 363]\n",
    "        alpha_prior = 20.0\n",
    "        beta_prior = 0.25\n",
    "        \n",
    "        T_eff = self.latent_states.shape[0] - 1\n",
    "        \n",
    "        # Règle de mise à jour conjuguée\n",
    "        alpha_post = alpha_prior + (T_eff / 2.0)\n",
    "        beta_post = beta_prior + (sse / 2.0)\n",
    "        \n",
    "        # 6. Échantillonnage\n",
    "        # Astuce NumPy: X ~ Gamma(a, scale=1/b) => 1/X ~ InvGamma(a, b)\n",
    "        gamma_draws = np.random.gamma(shape=alpha_post, scale=1.0/beta_post)\n",
    "        \n",
    "        # On évite la division par zéro (très improbable mais bonne pratique)\n",
    "        new_sigma = 1.0 / np.maximum(gamma_draws, 1e-10)\n",
    "        \n",
    "        # Mise à jour\n",
    "        self.theta['sigma'] = new_sigma\n",
    "\n",
    "    def update_mu_phi(self):\n",
    "        \"\"\"\n",
    "        Step 7: Tirage de mu et Phi conditionnellement aux états latents.\n",
    "        \n",
    "        Effectue une régression bayésienne sur les états latents h_t (self.latent_states).\n",
    "        Gère la contrainte de stationnarité pour Phi via une Normale Tronquée.\n",
    "        \n",
    "        Sources:\n",
    "        - [cite: 27] Draw mu, Phi conditional on state variables... acceptance sampling.\n",
    "        -[cite: 164]...performed in logarithms (géré par l'usage de latent_states).\n",
    "        - [cite: 362] Prior mu ~ N(0.4, 2).\n",
    "        - [cite: 363] Prior Phi ~ N(0.985, 0.001) tronqué sur (-1, 1).\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Récupération des États Latents (Gaussian h_t)\n",
    "        # Changement critique: utilisation de latent_states\n",
    "        h_curr = self.latent_states[:-1] # (T-1, n, p)\n",
    "        h_next = self.latent_states[1:]  # (T-1, n, p)\n",
    "        T_eff = h_curr.shape[0]\n",
    "        \n",
    "        # Variance actuelle sigma^2 (n, p)\n",
    "        sigma_sq = self.theta['sigma'] \n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # A. Mise à jour de MU (Moyenne Long Terme)\n",
    "        # Modèle: (h_{t+1} - Phi*h_t) = (1 - Phi)*mu + eta_t\n",
    "        # -------------------------------------------------------\n",
    "        phi = self.theta['phi'] # (n, p)\n",
    "        \n",
    "        # Prior Mu [cite: 362]\n",
    "        mu_prior_mean = 0.4\n",
    "        mu_prior_var = 2.0\n",
    "        mu_prior_prec = 1.0 / mu_prior_var\n",
    "        \n",
    "        # Pseudo-Variable dépendante Y\n",
    "        y_mu = h_next - phi[np.newaxis, :, :] * h_curr # (T-1, n, p)\n",
    "        \n",
    "        # Regresseur constant X = (1 - Phi)\n",
    "        X_mu = (1.0 - phi) # (n, p)\n",
    "        \n",
    "        # Précision et Moyenne pondérée (Likelihood terms)\n",
    "        # Somme sur T des précisions: T * X^2 / sigma^2\n",
    "        lik_prec_mu = T_eff * (X_mu**2) / sigma_sq\n",
    "        \n",
    "        # Somme sur T des (X * Y / sigma^2)\n",
    "        lik_mean_term_mu = (X_mu / sigma_sq) * np.sum(y_mu, axis=0)\n",
    "        \n",
    "        # Postérieur Mu\n",
    "        post_prec_mu = mu_prior_prec + lik_prec_mu\n",
    "        post_var_mu = 1.0 / post_prec_mu\n",
    "        post_mean_mu = post_var_mu * (mu_prior_prec * mu_prior_mean + lik_mean_term_mu)\n",
    "        \n",
    "        # Tirage Normal\n",
    "        self.theta['mu'] = np.random.normal(loc=post_mean_mu, scale=np.sqrt(post_var_mu))\n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # B. Mise à jour de PHI (Autocorrélation)\n",
    "        # Modèle: (h_{t+1} - mu) = Phi * (h_t - mu) + eta_t\n",
    "        # -------------------------------------------------------\n",
    "        mu = self.theta['mu'] # (n, p) (Nouvelle valeur)\n",
    "        \n",
    "        # Prior Phi [cite: 363]\n",
    "        phi_prior_mean = 0.985\n",
    "        phi_prior_var = 0.001\n",
    "        phi_prior_prec = 1.0 / phi_prior_var\n",
    "        \n",
    "        # Centrage des données (X = h_t - mu)\n",
    "        x_phi = h_curr - mu[np.newaxis, :, :]\n",
    "        y_phi = h_next - mu[np.newaxis, :, :]\n",
    "        \n",
    "        # Statistiques suffisantes (Sommes des produits croisés)\n",
    "        sum_x_sq = np.sum(x_phi**2, axis=0)      # sum(X^2)\n",
    "        sum_xy = np.sum(x_phi * y_phi, axis=0)   # sum(X*Y)\n",
    "        \n",
    "        # Termes de vraisemblance\n",
    "        lik_prec_phi = sum_x_sq / sigma_sq\n",
    "        lik_mean_term_phi = sum_xy / sigma_sq\n",
    "        \n",
    "        # Postérieur Phi (Non Tronqué)\n",
    "        post_prec_phi = phi_prior_prec + lik_prec_phi\n",
    "        post_var_phi = 1.0 / post_prec_phi\n",
    "        post_mean_phi = post_var_phi * (phi_prior_prec * phi_prior_mean + lik_mean_term_phi)\n",
    "        post_std_phi = np.sqrt(post_var_phi)\n",
    "        \n",
    "        # Tirage Tronqué sur (-1, 1) pour stationnarité\n",
    "        # Calcul des bornes normalisées pour truncnorm (a, b)\n",
    "        # Z = (X - mean) / std\n",
    "        a_clip = (-1.0 - post_mean_phi) / post_std_phi\n",
    "        b_clip = (1.0 - post_mean_phi) / post_std_phi\n",
    "        \n",
    "        # Utilisation de scipy.stats.truncnorm (vectorisé)\n",
    "        self.theta['phi'] = stats.truncnorm.rvs(\n",
    "            a_clip, \n",
    "            b_clip, \n",
    "            loc=post_mean_phi, \n",
    "            scale=post_std_phi, \n",
    "            size=mu.shape\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1faa8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnivariateLoadingSSM(ssm.StateSpaceModel):\n",
    "    \"\"\"\n",
    "    Modèle espace d'état pour les chargements d'un seul actif i.\n",
    "    Transition: Eq (2) [cite: 89]\n",
    "    Observation: Eq (5) avec scaling Eq (6)-(7) [cite: 146, 156]\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, phi, sigma_eta, z, zeta, data_x):\n",
    "        \"\"\"\n",
    "        :param mu: Moyenne long terme (scalaire ou vecteur taille p)\n",
    "        :param phi: Autocorrélation (scalaire ou diag)\n",
    "        :param sigma_eta: Variance du bruit de transition (scalaire ou diag)\n",
    "        :param z: Facteurs communs (T, p) [cite: 150]\n",
    "        :param zeta: Variable de mélange (T,) [cite: 220]\n",
    "        :param data_x: Données observées transformées x_it (T,) [cite: 5]\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.phi = phi\n",
    "        self.sigma_eta = sigma_eta\n",
    "        self.z = z\n",
    "        self.zeta = zeta\n",
    "        self.data_x = data_x\n",
    "        \n",
    "    def PX0(self):\n",
    "        \"\"\"Distribution initiale de lambda_{i,1} \"\"\"\n",
    "        # Stationnaire: N(mu, sigma^2 / (1-phi^2))\n",
    "        var_0 = self.sigma_eta / (1 - self.phi**2)\n",
    "        return distributions.Normal(loc=self.mu, scale=np.sqrt(var_0))\n",
    "\n",
    "    def PX(self, t, xp):\n",
    "        \"\"\"Transition lambda_{i, t} | lambda_{i, t-1} (Eq 2) \"\"\"\n",
    "        # AR(1): mu + phi * (prev - mu) + noise\n",
    "        mean = self.mu + self.phi * (xp - self.mu)\n",
    "        return distributions.Normal(loc=mean, scale=np.sqrt(self.sigma_eta))\n",
    "\n",
    "    def PY(self, t, xp, x):\n",
    "        \"\"\"\n",
    "        Densité d'observation p(x_it | lambda_{it}, z_t, zeta_t)\n",
    "        Utilise les équations de scaling (6) et (7).\n",
    "        \"\"\"\n",
    "        # 1. Récupérer les covariables au temps t\n",
    "        z_t = self.z[t]       # Facteurs (p,)\n",
    "        zeta_t = self.zeta[t] # Variable de mélange (scalaire)\n",
    "        obs = self.data_x[t]  # Donnée x_it\n",
    "        \n",
    "        # 2. Calcul du scaling \n",
    "        # Dans le cas p=1 (un seul facteur par loading pour simplifier l'exemple)\n",
    "        # Si p > 1, x est un vecteur et il faut sommer x^2\n",
    "        # Scaling factor: D = 1 + lambda'lambda\n",
    "        # Note: x ici est la particule courante représentant lambda_{it}\n",
    "        \n",
    "        # Gestion vectorielle pour les particules (N_particles,)\n",
    "        lam_sq = x**2 \n",
    "        scaling_factor = np.sqrt(1.0 + lam_sq) \n",
    "        \n",
    "        # 3. Chargements et variance transformés \n",
    "        # tilde_lambda = lambda / sqrt(1 + lambda^2)\n",
    "        tilde_lambda = x / scaling_factor\n",
    "        \n",
    "        # sigma_eps = 1 / sqrt(1 + lambda^2)\n",
    "        sigma_eps = 1.0 / scaling_factor\n",
    "        \n",
    "        # 4. Moyenne et Variance conditionnelles de x_it\n",
    "        # x_it = sqrt(zeta) * (tilde_lambda * z_t + sigma_eps * epsilon)\n",
    "        # Donc x_it ~ N( Mean, Var )\n",
    "        \n",
    "        # Moyenne = sqrt(zeta) * tilde_lambda * z_t\n",
    "        mean_obs = np.sqrt(zeta_t) * tilde_lambda * z_t\n",
    "        \n",
    "        # Ecart-type = sqrt(zeta) * sigma_eps\n",
    "        scale_obs = np.sqrt(zeta_t) * sigma_eps\n",
    "        \n",
    "        return distributions.Normal(loc=mean_obs, scale=scale_obs).logpdf(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc07062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qrt_foot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
