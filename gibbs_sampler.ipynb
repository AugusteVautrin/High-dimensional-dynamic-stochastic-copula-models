{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54ccccc9",
   "metadata": {},
   "source": [
    "# Implementation of the Gibbs sampler from the section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c247f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from particles import state_space_models as ssm\n",
    "from particles import distributions as dists\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import particles\n",
    "from particles import distributions\n",
    "from scipy import stats\n",
    "from particles import mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fca90b",
   "metadata": {},
   "source": [
    "## Step 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a676f8",
   "metadata": {},
   "source": [
    "On ne pourra pas directement utilisé le mcmc.ParticleGibbs de particules car l'un des principales aventage de l'augmented Gibbs est que conditionnelement au facteur on peut faire chaque actif est indépendant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorCopulaGibbs(mcmc.GenericGibbs):\n",
    "    \"\"\"\n",
    "    Échantillonneur de Gibbs pour le modèle de Copule Factorielle Dynamique.\n",
    "    Basé sur l'Appendix A de Creal & Tsay (2015).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, n_factors, prior, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data = data # u_it\n",
    "        self.n = data.shape[1]\n",
    "        self.T = data.shape[0]\n",
    "        self.p = n_factors\n",
    "        \n",
    "        # Initialisation des paramètres Theta \n",
    "        self.theta = prior.sample_initial() \n",
    "        # Contient: mu, phi, sigma, nu (degrees of freedom), beta\n",
    "        \n",
    "        # Initialisation des variables latentes \n",
    "        self.z = np.random.normal(size=(self.T, self.p)) # Facteurs communs\n",
    "        self.zeta = np.ones((self.T, self.n))            # Variables de mélange\n",
    "        self.lambdas = np.zeros((self.T, self.n, self.p)) # Chargements (State)\n",
    "        \n",
    "        # Pour le Particle Gibbs\n",
    "        self.ssm_cls = StochasticLoadingsSSM\n",
    "        # Stockage de la trajectoire des particules (pour conditional SMC)\n",
    "        self.lambda_path = None \n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Une itération complète du Gibbs Sampler (Appendix A)\"\"\"\n",
    "        \n",
    "        # Step 1: Missing Data \n",
    "        # On travaille avec des données complètes pour cet exemple\n",
    "        \n",
    "        # Step 2: Variables de mélange (Zeta)\n",
    "        self.update_zeta()\n",
    "        \n",
    "        # Step 3: Degrees of Freedom (nu)\n",
    "        self.update_nu()\n",
    "        \n",
    "        # Step 4: Facteurs Communs (z_t)\n",
    "        self.update_z()\n",
    "        \n",
    "        # Step 5: State Variables (Lambda)\n",
    "        self.update_lambda_pg()\n",
    "        \n",
    "        # Step 6: Bruit VAR (Sigma) \n",
    "        self.update_sigma()\n",
    "        \n",
    "        # Step 7: Paramètres VAR (mu, Phi)\n",
    "        self.update_mu_phi()\n",
    "        \n",
    "    def update_zeta(self):\n",
    "        \"\"\"\n",
    "        Step 2: Independence Metropolis-Hastings pour zeta.\n",
    "        \n",
    "        \"\"\"\n",
    "        pass \n",
    "\n",
    "    def update_nu(self):\n",
    "        \"\"\"\n",
    "        Step 3: Random-walk Metropolis pour nu.\n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update_z(self):\n",
    "        \"\"\"\n",
    "        Step 4: Gibbs standard pour z_t.\n",
    "        Mise à jour des facteurs latents communs.\n",
    "        \n",
    "        Sources:\n",
    "        - Distribution conditionnelle z_t ~ N(Mean, Var)\n",
    "        - Calcul de sigma_it et lambda_tilde_it\n",
    "        - Structure de l'inverse de la matrice de corrélation\n",
    "        \"\"\"\n",
    "        T, n = self.data.shape\n",
    "        p = self.p  # Nombre de facteurs\n",
    "        \n",
    "        # 1. Préparation des conteneurs\n",
    "        new_z = np.zeros((T, p))\n",
    "        I_p = np.eye(p)\n",
    "        \n",
    "        # On boucle sur le temps car les paramètres Lambda changent à chaque t\n",
    "        for t in range(T):\n",
    "            # --- A. Récupération des variables au temps t ---\n",
    "            # Données x_it (imputées/transformées)\n",
    "            x_t = self.data[t]  # (n,)\n",
    "            \n",
    "            # Chargements latents lambda_it \n",
    "            # self.lambdas est de forme (T, n, p)\n",
    "            lambda_t = self.lambdas[t] # (n, p)\n",
    "            \n",
    "            # Variables de mélange zeta_t (pour Student-t/Grouped-t)\n",
    "            # Si le modèle est Gaussien, zeta_t = 1 partout.\n",
    "            zeta_t = self.zeta[t] # (n,)\n",
    "\n",
    "            # --- B. Calcul des paramètres redimensionnés (Scaling) ---\n",
    "            # Selon , les paramètres utilisés dans la copule \n",
    "            # (tilde_lambda et sigma) dépendent de l'état latent lambda.\n",
    "            \n",
    "            # Calcul de la norme au carré de lambda pour chaque série i\n",
    "            # lambda_sq = sum(lambda_{it}^2)\n",
    "            lam_sq_norm = np.sum(lambda_t**2, axis=1) # (n,)\n",
    "            \n",
    "            # Variance idiosyncratique sigma_{it}^2 \n",
    "            # sigma^2 = 1 / (1 + lambda'lambda)\n",
    "            sigma_sq_t = 1.0 / (1.0 + lam_sq_norm) # (n,)\n",
    "            sigma_t = np.sqrt(sigma_sq_t)          # (n,)\n",
    "            \n",
    "            # Chargements redimensionnés tilde_lambda \n",
    "            # tilde_lambda = lambda / sqrt(1 + lambda'lambda) = lambda * sigma\n",
    "            lambda_tilde_t = lambda_t * sigma_t[:, np.newaxis] # (n, p)\n",
    "            \n",
    "            # --- C. Construction de la \"Donnée Transformée\" ---\n",
    "            # Selon Step 4 : x_dot = x / sqrt(zeta)\n",
    "            # Cela normalise la variance induite par la variable de mélange\n",
    "            x_dot_t = x_t / np.sqrt(zeta_t) # (n,)\n",
    "\n",
    "            # --- D. Calcul de la Moyenne et Variance Postérieure ---\n",
    "            # Le prior sur z_t est N(0, I_p).\n",
    "            # La vraisemblance est x_dot ~ N(tilde_lambda * z, D)\n",
    "            # où D est diagonale avec éléments sigma_sq_t.\n",
    "            \n",
    "            # Precision Matrix = I_p + C' D^-1 C\n",
    "            # Ici C = lambda_tilde. D^-1 = diag(1/sigma^2).\n",
    "            \n",
    "            # Astuce numérique : tilde_lambda' D^-1 tilde_lambda\n",
    "            # revient à : lambda' lambda (car tilde_lambda = lambda * sigma)\n",
    "            # Preuve: (lambda*sigma)' * (1/sigma^2) * (lambda*sigma) = lambda' * lambda\n",
    "            \n",
    "            # Calcul de la matrice de précision du likelihood\n",
    "            # precision_data = lambda_t.T @ lambda_t # Ce serait l'astuce\n",
    "            # Mais restons fidèles à la notation de l'article pour la clarté :\n",
    "            \n",
    "            # D_inv est un vecteur (diagonale inverse)\n",
    "            D_inv_diag = 1.0 / sigma_sq_t # (n,)\n",
    "            \n",
    "            # Terme C' D^-1\n",
    "            # On multiplie chaque colonne de lambda_tilde par D_inv\n",
    "            Ct_Dinv = lambda_tilde_t.T * D_inv_diag[np.newaxis, :] # (p, n)\n",
    "            \n",
    "            # Precision Postérieure = I + C' D^-1 C\n",
    "            # terme entre crochets\n",
    "            precision_post = I_p + Ct_Dinv @ lambda_tilde_t # (p, p)\n",
    "            \n",
    "            # Covariance Postérieure (Sigma_z)\n",
    "            # On utilise Cholesky pour la stabilité numérique et pour le tirage ensuite\n",
    "            # L = cholesky(Precision) -> Precision = L L.T\n",
    "            try:\n",
    "                L_prec = linalg.cholesky(precision_post, lower=True)\n",
    "                # Pour inverser, on résout le système linéaire. \n",
    "                # Cov = Prec^-1.\n",
    "                # Mais on a juste besoin de résoudre Mean = Cov * Terme_Lineaire\n",
    "                # => Prec * Mean = Terme_Lineaire\n",
    "            except linalg.LinAlgError:\n",
    "                # Fallback en cas de problèmes numériques rares\n",
    "                L_prec = linalg.cholesky(precision_post + 1e-6 * np.eye(p), lower=True)\n",
    "\n",
    "            # Moyenne Postérieure (Mu_z)\n",
    "            # Mean = Cov_post * (C' D^-1 x_dot)\n",
    "            # terme de droite dans la moyenne\n",
    "            linear_term = Ct_Dinv @ x_dot_t # (p,)\n",
    "            \n",
    "            # Résolution de : Precision * Mean = linear_term\n",
    "            # On utilise cholesky_solve pour la rapidité\n",
    "            mu_post = linalg.cho_solve((L_prec, True), linear_term)\n",
    "            \n",
    "            # --- E. Tirage aléatoire (Draw) ---\n",
    "            # z_t = mu_post + chol(Cov_post) * epsilon\n",
    "            # Note: chol(Cov) = inv(L_prec.T)\n",
    "            \n",
    "            epsilon = np.random.normal(size=p)\n",
    "            \n",
    "            # On résout L_prec.T * z_noise = epsilon pour obtenir z_noise ~ N(0, Cov)\n",
    "            # C'est plus stable que d'inverser explicitement\n",
    "            z_noise = linalg.solve_triangular(L_prec.T, epsilon, lower=False)\n",
    "            \n",
    "            new_z[t] = mu_post + z_noise\n",
    "\n",
    "        # Mise à jour de l'attribut de la classe\n",
    "        self.z = new_z\n",
    "\n",
    "    def update_lambda_pg(self):\n",
    "        \"\"\"\n",
    "        Step 5: Draw state variables Lambda using Particle Gibbs.\n",
    "        [cite: 21, 302]\n",
    "        \n",
    "        Assumption: Loadings are independent across i conditional on z_t[cite: 23].\n",
    "        \"\"\"\n",
    "        T, n = self.data.shape\n",
    "        p = self.p\n",
    "        \n",
    "        # Conteneur pour les nouvelles trajectoires\n",
    "        new_lambdas = np.zeros((T, n, p))\n",
    "        \n",
    "        # Paramètres courants (supposés diagonaux pour permettre le traitement par actif)\n",
    "        mu_vec = self.theta['mu']      # (n, p) ou (n,)\n",
    "        phi_vec = self.theta['phi']    # (n, p) ou (n,)\n",
    "        sigma_vec = self.theta['sigma'] # (n, p) ou (n,)\n",
    "        \n",
    "        # Boucle séquentielle sur chaque actif (série temporelle)\n",
    "        # [cite: 307] \"we draw each path lambda_{i,1:T} separately\"\n",
    "        for i in range(n):\n",
    "            \n",
    "            # 1. Extraction des paramètres spécifiques à l'actif i\n",
    "            # On suppose ici p=1 pour simplifier l'indexation, ou que les p facteurs sont indépendants\n",
    "            mu_i = mu_vec[i]\n",
    "            phi_i = phi_vec[i]\n",
    "            sigma_i = sigma_vec[i]\n",
    "            \n",
    "            # Données spécifiques à i\n",
    "            data_i = self.data_x[:, i] # Données transformées x_it (T,)\n",
    "            \n",
    "            # 2. Instanciation du SSM pour l'actif i\n",
    "            # On passe les facteurs communs z (T, p) qui sont conditionnés \n",
    "            ssm_i = UnivariateLoadingSSM(\n",
    "                mu=mu_i,\n",
    "                phi=phi_i,\n",
    "                sigma_eta=sigma_i,\n",
    "                z=self.z,          # Facteurs communs fixes pour cette étape\n",
    "                zeta=self.zeta[:, i], # Variables de mélange pour i\n",
    "                data_x=data_i      # Observations\n",
    "            )\n",
    "            \n",
    "            # 3. Exécution du Filtre Particulaire (SMC)\n",
    "            # [cite: 305] \"Discrete approximation through a set of particles\"\n",
    "            # N=100 selon le papier [cite: 345]\n",
    "            fk_model = ssm.Bootstrap(ssm=ssm_i, data=data_i)\n",
    "            pf = particles.SMC(fk=fk_model, N=100, store_history=True)\n",
    "            pf.run()\n",
    "            \n",
    "            # 4. Backward Sampling (FFBSm)\n",
    "            # [cite: 334] \"Draw a path ... using backwards sampling algorithm\"\n",
    "            # [cite: 343] \"The draw is a draw from the full-conditional distribution\"\n",
    "            # La méthode backward_sampling de particles retourne une liste de (T,)\n",
    "            traj = pf.hist.backward_sampling(1)\n",
    "            \n",
    "            # 5. Stockage\n",
    "            # traj est une liste de ndarrays, on convertit en array (T,)\n",
    "            new_lambdas[:, i, 0] = np.array(traj).flatten()\n",
    "            \n",
    "        # Mise à jour de l'état global\n",
    "        self.lambdas = new_lambdas\n",
    "\n",
    "    def update_sigma(self):\n",
    "        \"\"\"\n",
    "        Step 6: Tirage de la variance d'état Sigma (Inverse Gamma).\n",
    "        \n",
    "        Met à jour la variance des chocs de transition Lambda conditionnellement\n",
    "        à la trajectoire des états Lambda_{1:T}.\n",
    "        \n",
    "        Sources:\n",
    "        [cite_start]- [cite: 25] Draw Sigma conditional on state variables Lambda and (mu, Phi).\n",
    "        [cite_start]- [cite: 26] With Inverse Gamma prior, posterior is Inverse Gamma (Standard).\n",
    "        [cite_start]- [cite: 363] Prior utilisé dans l'article: InvGamma(20, 0.25).\n",
    "        [cite_start]- [cite: 89] Equation de transition: Lambda_{t+1} = mu + Phi(Lambda_t - mu) + eta\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Calcul des résidus du processus AR(1)\n",
    "        # [cite_start]L'équation (2) [cite: 89] définit la dynamique des états.\n",
    "        # Nous calculons l'erreur de prédiction entre t et t+1.\n",
    "        \n",
    "        # lambda_{2:T} (Observed Next State)\n",
    "        lambda_next = self.lambdas[1:] # Shape: (T-1, n, p)\n",
    "        \n",
    "        # lambda_{1:T-1} (Current State)\n",
    "        lambda_curr = self.lambdas[:-1] # Shape: (T-1, n, p)\n",
    "        \n",
    "        # Paramètres courants (avec broadcasting pour la dimension temporelle)\n",
    "        # mu et phi sont de forme (n, p) -> on ajoute l'axe temps: (1, n, p)\n",
    "        mu = self.theta['mu'][np.newaxis, :, :]\n",
    "        phi = self.theta['phi'][np.newaxis, :, :]\n",
    "        \n",
    "        # Prédiction : mu + phi * (lambda_t - mu)\n",
    "        lambda_pred = mu + phi * (lambda_curr - mu)\n",
    "        \n",
    "        # Résidus eta_t ~ N(0, Sigma)\n",
    "        residuals = lambda_next - lambda_pred # Shape: (T-1, n, p)\n",
    "        \n",
    "        # 2. Calcul de la Somme des Carrés des Erreurs (SSE)\n",
    "        # On somme sur l'axe temporel (axis=0)\n",
    "        sse = np.sum(residuals**2, axis=0) # Shape: (n, p)\n",
    "        \n",
    "        # 3. Paramètres du Postérieur (Inverse Gamma)\n",
    "        # [cite_start]Prior donné dans l'application[cite: 363]: alpha=20, beta=0.25\n",
    "        alpha_prior = 20.0\n",
    "        beta_prior = 0.25\n",
    "        \n",
    "        # Nombre d'observations effectives pour la transition\n",
    "        T_eff = self.lambdas.shape[0] - 1\n",
    "        \n",
    "        # Mise à jour des hyperparamètres (Règle standard conjuguée)\n",
    "        # alpha_post = alpha_prior + T/2\n",
    "        alpha_post = alpha_prior + (T_eff / 2.0)\n",
    "        \n",
    "        # beta_post = beta_prior + SSE/2\n",
    "        beta_post = beta_prior + (sse / 2.0)\n",
    "        \n",
    "        # 4. Échantillonnage (Sampling)\n",
    "        # NumPy n'a pas de np.random.invgamma direct paramétré en alpha/beta.\n",
    "        # On utilise la relation: Si X ~ Gamma(alpha, beta_rate), alors 1/X ~ InvGamma(alpha, beta_rate).\n",
    "        # Note: np.random.gamma prend (shape, scale). scale = 1/rate = 1/beta_post.\n",
    "        \n",
    "        gamma_draws = np.random.gamma(shape=alpha_post, scale=1.0/beta_post)\n",
    "        \n",
    "        # On inverse pour obtenir l'Inverse Gamma\n",
    "        new_sigma = 1.0 / gamma_draws # Shape: (n, p)\n",
    "        \n",
    "        # Mise à jour du dictionnaire de paramètres\n",
    "        self.theta['sigma'] = new_sigma\n",
    "\n",
    "    def update_mu_phi(self):\n",
    "        \"\"\"\n",
    "        Step 7: Tirage de mu et Phi conditionnellement aux états Lambda et Sigma.\n",
    "        \n",
    "        Met à jour la moyenne long terme (mu) et l'autocorrélation (Phi)\n",
    "        des processus AR(1) des états latents.\n",
    "        \n",
    "        Transition Model: Lambda_{t+1} = mu + Phi * (Lambda_t - mu) + eta_t\n",
    "        eta_t ~ N(0, Sigma)\n",
    "        \n",
    "        Sources:\n",
    "        -  \"Draw mu, Phi... acceptance sampling for truncated normal... standard.\"\n",
    "        - [cite: 362] Prior mu ~ N(0.4, 2)\n",
    "        - [cite: 363] Prior Phi ~ N(0.985, 0.001) truncated to stationarity region (-1, 1).\n",
    "        - [cite: 362] Assumption: Phi and Sigma are diagonal (indépendance élément par élément).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Données: Séparation des états en t (current) et t+1 (next)\n",
    "        # Dimensions: lam_curr (T-1, n, p), lam_next (T-1, n, p)\n",
    "        lam_curr = self.lambdas[:-1]\n",
    "        lam_next = self.lambdas[1:]\n",
    "        T_eff = lam_curr.shape[0] # T-1\n",
    "        \n",
    "        # Récupération de la variance courante sigma^2 (n, p)\n",
    "        # Diffusée (broadcasted) pour les calculs\n",
    "        sigma_sq = self.theta['sigma'] \n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # A. Mise à jour de MU (Moyenne Long Terme)\n",
    "        # -------------------------------------------------------\n",
    "        # On réécrit l'équation: Lambda_{t+1} - Phi*Lambda_t = (1 - Phi)*mu + eta_t\n",
    "        # Forme: Y = X*mu + eta. Ici X = (1-Phi) est constant dans le temps.\n",
    "        \n",
    "        # Paramètres courants de Phi (n, p)\n",
    "        phi = self.theta['phi']\n",
    "        \n",
    "        # Priors pour Mu [cite: 362]\n",
    "        mu_prior_mean = 0.4\n",
    "        mu_prior_var = 2.0\n",
    "        mu_prior_prec = 1.0 / mu_prior_var\n",
    "        \n",
    "        # 1. Construction des \"pseudo-données\"\n",
    "        # Y_t = Lambda_{t+1} - Phi * Lambda_t\n",
    "        y_mu = lam_next - phi[np.newaxis, :, :] * lam_curr # (T-1, n, p)\n",
    "        \n",
    "        # \"Regresseur\" constant A = (1 - Phi)\n",
    "        A = (1.0 - phi) # (n, p)\n",
    "        \n",
    "        # 2. Calcul des statistiques suffisantes (Précision et Moyenne pondérée)\n",
    "        # Likelihood Precision = sum(A^2 / sigma^2) = T * A^2 / sigma^2\n",
    "        lik_prec_mu = T_eff * (A**2) / sigma_sq\n",
    "        \n",
    "        # Likelihood Mean term = sum(A * Y_t / sigma^2) = (A / sigma^2) * sum(Y_t)\n",
    "        lik_mean_term_mu = (A / sigma_sq) * np.sum(y_mu, axis=0)\n",
    "        \n",
    "        # 3. Postérieur pour Mu (Conjugaison Normale-Normale)\n",
    "        post_prec_mu = mu_prior_prec + lik_prec_mu\n",
    "        post_var_mu = 1.0 / post_prec_mu\n",
    "        post_mean_mu = post_var_mu * (mu_prior_prec * mu_prior_mean + lik_mean_term_mu)\n",
    "        \n",
    "        # 4. Tirage de Mu\n",
    "        self.theta['mu'] = np.random.normal(loc=post_mean_mu, scale=np.sqrt(post_var_mu))\n",
    "        \n",
    "        # -------------------------------------------------------\n",
    "        # B. Mise à jour de PHI (Autocorrélation) - Normale Tronquée\n",
    "        # -------------------------------------------------------\n",
    "        # On réécrit l'équation: (Lambda_{t+1} - mu) = Phi * (Lambda_t - mu) + eta_t\n",
    "        # Forme: Y = Phi*X + eta (Régression sans constante passant par l'origine)\n",
    "        \n",
    "        # Nouvelle valeur de mu (n, p) venant d'être tirée\n",
    "        mu = self.theta['mu']\n",
    "        \n",
    "        # Priors pour Phi [cite: 363]\n",
    "        phi_prior_mean = 0.985\n",
    "        phi_prior_var = 0.001\n",
    "        phi_prior_prec = 1.0 / phi_prior_var\n",
    "        \n",
    "        # 1. Données centrées\n",
    "        # X_t = Lambda_t - mu\n",
    "        x_phi = lam_curr - mu[np.newaxis, :, :]\n",
    "        # Y_t = Lambda_{t+1} - mu\n",
    "        y_phi = lam_next - mu[np.newaxis, :, :]\n",
    "        \n",
    "        # 2. Statistiques suffisantes\n",
    "        # sum_x_sq = sum(X_t^2)\n",
    "        sum_x_sq = np.sum(x_phi**2, axis=0)\n",
    "        # sum_xy = sum(X_t * Y_t)\n",
    "        sum_xy = np.sum(x_phi * y_phi, axis=0)\n",
    "        \n",
    "        # Likelihood Precision = sum(X^2) / sigma^2\n",
    "        lik_prec_phi = sum_x_sq / sigma_sq\n",
    "        \n",
    "        # Likelihood Mean term = sum(X*Y) / sigma^2\n",
    "        lik_mean_term_phi = sum_xy / sigma_sq\n",
    "        \n",
    "        # 3. Postérieur non tronqué (Conjugaison Normale-Normale)\n",
    "        post_prec_phi = phi_prior_prec + lik_prec_phi\n",
    "        post_var_phi = 1.0 / post_prec_phi\n",
    "        post_mean_phi = post_var_phi * (phi_prior_prec * phi_prior_mean + lik_mean_term_phi)\n",
    "        post_std_phi = np.sqrt(post_var_phi)\n",
    "        \n",
    "        # 4. Tirage Tronqué sur (-1, 1) \n",
    "        # \"We use acceptance sampling for the truncated normal... which is standard.\"\n",
    "        # Pour l'efficacité et la robustesse vectorisée, on utilise scipy.stats.truncnorm\n",
    "        # Paramétrage de truncnorm: a et b sont les bornes normalisées\n",
    "        \n",
    "        a_clip = (-1.0 - post_mean_phi) / post_std_phi\n",
    "        b_clip = (1.0 - post_mean_phi) / post_std_phi\n",
    "        \n",
    "        self.theta['phi'] = stats.truncnorm.rvs(\n",
    "            a_clip, \n",
    "            b_clip, \n",
    "            loc=post_mean_phi, \n",
    "            scale=post_std_phi, \n",
    "            size=mu.shape\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1faa8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnivariateLoadingSSM(ssm.StateSpaceModel):\n",
    "    \"\"\"\n",
    "    Modèle espace d'état pour les chargements d'un seul actif i.\n",
    "    Transition: Eq (2) [cite: 89]\n",
    "    Observation: Eq (5) avec scaling Eq (6)-(7) [cite: 146, 156]\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, phi, sigma_eta, z, zeta, data_x):\n",
    "        \"\"\"\n",
    "        :param mu: Moyenne long terme (scalaire ou vecteur taille p)\n",
    "        :param phi: Autocorrélation (scalaire ou diag)\n",
    "        :param sigma_eta: Variance du bruit de transition (scalaire ou diag)\n",
    "        :param z: Facteurs communs (T, p) [cite: 150]\n",
    "        :param zeta: Variable de mélange (T,) [cite: 220]\n",
    "        :param data_x: Données observées transformées x_it (T,) [cite: 5]\n",
    "        \"\"\"\n",
    "        self.mu = mu\n",
    "        self.phi = phi\n",
    "        self.sigma_eta = sigma_eta\n",
    "        self.z = z\n",
    "        self.zeta = zeta\n",
    "        self.data_x = data_x\n",
    "        \n",
    "    def PX0(self):\n",
    "        \"\"\"Distribution initiale de lambda_{i,1} \"\"\"\n",
    "        # Stationnaire: N(mu, sigma^2 / (1-phi^2))\n",
    "        var_0 = self.sigma_eta / (1 - self.phi**2)\n",
    "        return distributions.Normal(loc=self.mu, scale=np.sqrt(var_0))\n",
    "\n",
    "    def PX(self, t, xp):\n",
    "        \"\"\"Transition lambda_{i, t} | lambda_{i, t-1} (Eq 2) \"\"\"\n",
    "        # AR(1): mu + phi * (prev - mu) + noise\n",
    "        mean = self.mu + self.phi * (xp - self.mu)\n",
    "        return distributions.Normal(loc=mean, scale=np.sqrt(self.sigma_eta))\n",
    "\n",
    "    def PY(self, t, xp, x):\n",
    "        \"\"\"\n",
    "        Densité d'observation p(x_it | lambda_{it}, z_t, zeta_t)\n",
    "        Utilise les équations de scaling (6) et (7).\n",
    "        \"\"\"\n",
    "        # 1. Récupérer les covariables au temps t\n",
    "        z_t = self.z[t]       # Facteurs (p,)\n",
    "        zeta_t = self.zeta[t] # Variable de mélange (scalaire)\n",
    "        obs = self.data_x[t]  # Donnée x_it\n",
    "        \n",
    "        # 2. Calcul du scaling \n",
    "        # Dans le cas p=1 (un seul facteur par loading pour simplifier l'exemple)\n",
    "        # Si p > 1, x est un vecteur et il faut sommer x^2\n",
    "        # Scaling factor: D = 1 + lambda'lambda\n",
    "        # Note: x ici est la particule courante représentant lambda_{it}\n",
    "        \n",
    "        # Gestion vectorielle pour les particules (N_particles,)\n",
    "        lam_sq = x**2 \n",
    "        scaling_factor = np.sqrt(1.0 + lam_sq) \n",
    "        \n",
    "        # 3. Chargements et variance transformés \n",
    "        # tilde_lambda = lambda / sqrt(1 + lambda^2)\n",
    "        tilde_lambda = x / scaling_factor\n",
    "        \n",
    "        # sigma_eps = 1 / sqrt(1 + lambda^2)\n",
    "        sigma_eps = 1.0 / scaling_factor\n",
    "        \n",
    "        # 4. Moyenne et Variance conditionnelles de x_it\n",
    "        # x_it = sqrt(zeta) * (tilde_lambda * z_t + sigma_eps * epsilon)\n",
    "        # Donc x_it ~ N( Mean, Var )\n",
    "        \n",
    "        # Moyenne = sqrt(zeta) * tilde_lambda * z_t\n",
    "        mean_obs = np.sqrt(zeta_t) * tilde_lambda * z_t\n",
    "        \n",
    "        # Ecart-type = sqrt(zeta) * sigma_eps\n",
    "        scale_obs = np.sqrt(zeta_t) * sigma_eps\n",
    "        \n",
    "        return distributions.Normal(loc=mean_obs, scale=scale_obs).logpdf(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc07062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_qrt_foot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
